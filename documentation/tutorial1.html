<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Tensor Theory and Mathematical Foundations</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        
        h1, h2, h3, h4 {
            color: #2c3e50;
            margin-top: 24px;
        }
        
        h1 {
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
            font-size: 2.5em;
            text-align: center;
        }
        
        h2 {
            border-left: 5px solid #3498db;
            padding-left: 10px;
            background-color: #ecf0f1;
            padding: 8px 12px;
        }
        
        .author-info {
            text-align: center;
            margin-bottom: 40px;
        }
        
        .definition {
            background-color: #e8f4fc;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin: 20px 0;
        }
        
        .note {
            background-color: #e8f8f5;
            border-left: 4px solid #1abc9c;
            padding: 15px;
            margin: 20px 0;
        }
        
        .formula {
            background-color: #f5f5f5;
            padding: 15px;
            margin: 20px 0;
            text-align: center;
            font-family: 'Cambria Math', Georgia, serif;
            font-size: 1.1em;
        }
        
        .section-separator {
            border-top: 1px dashed #bdc3c7;
            margin: 40px 0;
        }
        
        .github-link {
            display: inline-block;
            font-style: italic;
            color: #3498db;
            text-decoration: none;
        }
        
        .github-link:hover {
            text-decoration: underline;
        }
        
        .concept-container {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            margin: 20px 0;
        }
        
        .concept-box {
            flex: 1 1 300px;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            background-color: white;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        
        .concept-box h3 {
            margin-top: 0;
            border-bottom: 1px solid #eee;
            padding-bottom: 8px;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            
            .concept-box {
                flex: 1 1 100%;
            }
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>Chapter 1: Tensor Theory and Mathematical Foundations</h1>
        <div class="author-info">
            <h3>Author: Akshay Anand, PhD Candidate</h3>
            <p><strong>Florida State University (2021 - 20XX)</strong></p>
            <a href="https://github.com/anand-me" class="github-link">GitHub</a>
        </div>
    </header>

    <section>
        <h2>Introduction to Tensors</h2>
        <p>
            Tensors form the fundamental mathematical objects underlying modern deep learning frameworks like PyTorch. To truly understand neural networks, we must first develop a solid understanding of what tensors are, their properties, and how they behave mathematically.
        </p>

        <h3>Mathematical Definition</h3>
        <p>
            A tensor is a mathematical object that can be viewed as a generalization of scalars, vectors, and matrices to potentially higher dimensions. Formally, a tensor of rank (or order) \(n\) is an element of the tensor product of \(n\) vector spaces, each with its own coordinate system.
        </p>

        <div class="definition">
            <p>In the context of multidimensional arrays, we can define a tensor as follows:</p>
            <ul>
                <li><strong>Rank-0 Tensor (Scalar)</strong>: A single number, \(x \in \mathbb{R}\)</li>
                <li><strong>Rank-1 Tensor (Vector)</strong>: An ordered array of numbers, \(\mathbf{v} \in \mathbb{R}^n\)</li>
                <li><strong>Rank-2 Tensor (Matrix)</strong>: A rectangular array of numbers, \(\mathbf{M} \in \mathbb{R}^{m \times n}\)</li>
                <li><strong>Rank-3 Tensor</strong>: A cube-like array of numbers, \(\mathbf{T} \in \mathbb{R}^{l \times m \times n}\)</li>
                <li><strong>Rank-\(k\) Tensor</strong>: A \(k\)-dimensional array, \(\mathbf{T} \in \mathbb{R}^{d_1 \times d_2 \times \cdots \times d_k}\)</li>
            </ul>
        </div>

        <h3>Tensor Shape and Dimensionality</h3>
        <p>
            The <strong>shape</strong> of a tensor is a tuple that defines the number of elements along each dimension. For a tensor \(\mathbf{T} \in \mathbb{R}^{d_1 \times d_2 \times \cdots \times d_k}\), its shape is \((d_1, d_2, \ldots, d_k)\).
        </p>
        <p>
            The <strong>dimensionality</strong> (or rank) of a tensor refers to the number of indices required to uniquely identify an element within it. This corresponds to the length of the shape tuple.
        </p>

        <div class="concept-container">
            <div class="concept-box">
                <h3>Examples of Tensor Dimensions</h3>
                <ul>
                    <li><strong>Scalar</strong>: Shape () - requires 0 indices</li>
                    <li><strong>Vector of length 5</strong>: Shape (5) - requires 1 index</li>
                    <li><strong>3Ã—3 Matrix</strong>: Shape (3, 3) - requires 2 indices</li>
                    <li><strong>RGB Image (height 32, width 32)</strong>: Shape (32, 32, 3) - requires 3 indices</li>
                    <li><strong>Batch of 16 images</strong>: Shape (16, 32, 32, 3) - requires 4 indices</li>
                </ul>
            </div>
            <div class="concept-box">
                <h3>Tensor Types</h3>
                <p>In mathematical contexts, tensors can be categorized based on their transformation properties:</p>
                <ul>
                    <li><strong>Covariant tensors</strong>: Transform with the basis vectors</li>
                    <li><strong>Contravariant tensors</strong>: Transform inversely to the basis vectors</li>
                    <li><strong>Mixed tensors</strong>: Contain both covariant and contravariant components</li>
                </ul>
                <p>In computational contexts like PyTorch, tensors are categorized by their data type, which specifies the kind of numerical data they store:</p>
                <ul>
                    <li><strong>Floating-point types</strong>: <code>float32</code>, <code>float64</code> (double precision)</li>
                    <li><strong>Integer types</strong>: <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code></li>
                    <li><strong>Boolean type</strong>: <code>bool</code></li>
                    <li><strong>Complex types</strong>: <code>complex64</code>, <code>complex128</code></li>
                </ul>
            </div>
        </div>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>Tensor Algebra</h2>
        
        <h3>Tensor Addition and Subtraction</h3>
        <p>
            For tensors of identical shape, addition and subtraction are performed element-wise:
        </p>
        <div class="formula">
            \[(\mathbf{A} + \mathbf{B})_{i,j,k,\ldots} = \mathbf{A}_{i,j,k,\ldots} + \mathbf{B}_{i,j,k,\ldots}\]
            \[(\mathbf{A} - \mathbf{B})_{i,j,k,\ldots} = \mathbf{A}_{i,j,k,\ldots} - \mathbf{B}_{i,j,k,\ldots}\]
        </div>

        <h3>Scalar Multiplication</h3>
        <p>
            Multiplication of a tensor by a scalar applies the scalar to each element:
        </p>
        <div class="formula">
            \[(c\mathbf{A})_{i,j,k,\ldots} = c \cdot \mathbf{A}_{i,j,k,\ldots}\]
        </div>

        <h3>Element-wise Multiplication (Hadamard Product)</h3>
        <p>
            For tensors of identical shape, element-wise multiplication is:
        </p>
        <div class="formula">
            \[(\mathbf{A} \odot \mathbf{B})_{i,j,k,\ldots} = \mathbf{A}_{i,j,k,\ldots} \cdot \mathbf{B}_{i,j,k,\ldots}\]
        </div>

        <h3>Tensor Contraction and Einstein Notation</h3>
        <p>
            Tensor contraction involves summing over matching indices. Einstein notation provides a compact way to represent this:
        </p>
        <p>
            For matrices \(\mathbf{A} \in \mathbb{R}^{m \times n}\) and \(\mathbf{B} \in \mathbb{R}^{n \times p}\), their matrix product can be written as:
        </p>
        <div class="formula">
            \[\mathbf{C}_{ij} = \sum_{k} \mathbf{A}_{ik} \mathbf{B}_{kj}\]
        </div>
        <p>
            In Einstein notation, this becomes:
        </p>
        <div class="formula">
            \[\mathbf{C}_{ij} = \mathbf{A}_{ik} \mathbf{B}_{kj}\]
        </div>
        <p>
            where the repeated index \(k\) implies summation.
        </p>

        <h3>Tensor Product</h3>
        <p>
            The tensor product (outer product) of tensors \(\mathbf{A} \in \mathbb{R}^{d_1 \times \cdots \times d_n}\) and \(\mathbf{B} \in \mathbb{R}^{e_1 \times \cdots \times e_m}\) creates a new tensor \(\mathbf{C} \in \mathbb{R}^{d_1 \times \cdots \times d_n \times e_1 \times \cdots \times e_m}\):
        </p>
        <div class="formula">
            \[\mathbf{C}_{i_1, \ldots, i_n, j_1, \ldots, j_m} = \mathbf{A}_{i_1, \ldots, i_n} \cdot \mathbf{B}_{j_1, \ldots, j_m}\]
        </div>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>Advanced Tensor Operations</h2>
        
        <h3>Broadcasting</h3>
        <p>
            Broadcasting allows operations between tensors of different but compatible shapes. The general rules are:
        </p>
        <ol>
            <li>Dimensions are aligned from right to left</li>
            <li>Size-1 dimensions are stretched to match larger dimensions</li>
            <li>Missing dimensions are implicitly added with size 1</li>
        </ol>

        <div class="note">
            <p>
                Mathematically, if tensor \(\mathbf{A}\) has shape \((a_1, a_2, \ldots, a_n)\) and tensor \(\mathbf{B}\) has shape \((b_1, b_2, \ldots, b_m)\) with \(m < n\), then \(\mathbf{B}\) is treated as having shape \((1, \ldots, 1, b_1, b_2, \ldots, b_m)\) with \((n-m)\) leading ones.
            </p>
            <p>
                For dimensions where either \(a_i = 1\) or \(b_i = 1\), the corresponding value is duplicated to match the other tensor's dimension.
            </p>
        </div>

        <h3>Reduction Operations</h3>
        <p>
            Reduction operations collapse one or more dimensions of a tensor by applying an aggregation function:
        </p>
        <ul>
            <li><strong>Sum</strong>: \(\sum_{i} \mathbf{T}_{i,j,k,\ldots}\)</li>
            <li><strong>Mean</strong>: \(\frac{1}{N} \sum_{i} \mathbf{T}_{i,j,k,\ldots}\) where \(N\) is the size of dimension \(i\)</li>
            <li><strong>Max</strong>: \(\max_{i} \mathbf{T}_{i,j,k,\ldots}\)</li>
            <li><strong>Min</strong>: \(\min_{i} \mathbf{T}_{i,j,k,\ldots}\)</li>
        </ul>

        <h3>Tensor Decomposition</h3>
        <p>
            Tensor decomposition methods extend matrix factorization to higher-order tensors:
        </p>
        <ol>
            <li><strong>CANDECOMP/PARAFAC (CP) Decomposition</strong>: Represents a tensor as a sum of rank-one tensors</li>
            <li><strong>Tucker Decomposition</strong>: Decomposes a tensor into a core tensor multiplied by matrices along each mode</li>
            <li><strong>Tensor Train Decomposition</strong>: Represents a tensor as a train of contracted lower-order tensors</li>
        </ol>
        <p>
            For a 3D tensor \(\mathcal{X} \in \mathbb{R}^{I \times J \times K}\), CP decomposition represents it as:
        </p>
        <div class="formula">
            \[\mathcal{X} \approx \sum_{r=1}^{R} \mathbf{a}_r \circ \mathbf{b}_r \circ \mathbf{c}_r\]
        </div>
        <p>
            where \(\circ\) denotes the outer product, and \(\mathbf{a}_r \in \mathbb{R}^I\), \(\mathbf{b}_r \in \mathbb{R}^J\), and \(\mathbf{c}_r \in \mathbb{R}^K\).
        </p>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>Tensor Calculus</h2>
        
        <h3>Differentiation</h3>
        <p>
            The derivative of a scalar function \(f\) with respect to a tensor \(\mathbf{T}\) is a tensor of the same shape as \(\mathbf{T}\), where each element is the partial derivative of \(f\) with respect to the corresponding element of \(\mathbf{T}\):
        </p>
        <div class="formula">
            \[\frac{\partial f}{\partial \mathbf{T}}_{i,j,k,\ldots} = \frac{\partial f}{\partial \mathbf{T}_{i,j,k,\ldots}}\]
        </div>

        <h3>The Chain Rule</h3>
        <p>
            For tensor calculus, the chain rule states that if \(f = g(\mathbf{Y})\) and \(\mathbf{Y} = h(\mathbf{X})\), then:
        </p>
        <div class="formula">
            \[\frac{\partial f}{\partial \mathbf{X}} = \frac{\partial f}{\partial \mathbf{Y}} \frac{\partial \mathbf{Y}}{\partial \mathbf{X}}\]
        </div>
        <p>
            This is the foundational principle behind backpropagation in neural networks.
        </p>

        <h3>Jacobian Matrices</h3>
        <p>
            The Jacobian matrix represents the first-order partial derivatives of a vector-valued function. For a function \(\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m\), the Jacobian is an \(m \times n\) matrix:
        </p>
        <div class="formula">
            \[\mathbf{J}_\mathbf{f} = \begin{pmatrix}
            \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
            \vdots & \ddots & \vdots \\
            \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
            \end{pmatrix}\]
        </div>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>Tensors in Neural Networks</h2>
        
        <h3>Weight Tensors</h3>
        <p>
            In a neural network, weights are typically represented as tensors:
        </p>
        <ul>
            <li><strong>Fully Connected Layer</strong>: Weight matrix \(\mathbf{W} \in \mathbb{R}^{m \times n}\) where \(m\) is the output size and \(n\) is the input size</li>
            <li><strong>Convolutional Layer</strong>: Weight tensor \(\mathbf{W} \in \mathbb{R}^{o \times i \times k_h \times k_w}\) where \(o\) is the number of output channels, \(i\) is the number of input channels, and \(k_h, k_w\) are the kernel height and width</li>
        </ul>

        <h3>Data Tensors</h3>
        <p>
            Input data in neural networks is also represented as tensors:
        </p>
        <ul>
            <li><strong>Tabular Data</strong>: Matrix \(\mathbf{X} \in \mathbb{R}^{b \times f}\) where \(b\) is the batch size and \(f\) is the number of features</li>
            <li><strong>Images</strong>: Tensor \(\mathbf{X} \in \mathbb{R}^{b \times c \times h \times w}\) where \(b\) is the batch size, \(c\) is the number of channels, and \(h, w\) are the image height and width</li>
            <li><strong>Sequences</strong>: Tensor \(\mathbf{X} \in \mathbb{R}^{b \times s \times f}\) where \(b\) is the batch size, \(s\) is the sequence length, and \(f\) is the feature dimension</li>
        </ul>

        <h3>Activation Tensors</h3>
        <p>
            Activation functions operate element-wise on tensors. For a function \(\sigma\) and tensor \(\mathbf{Z}\):
        </p>
        <div class="formula">
            \[\sigma(\mathbf{Z})_{i,j,k,\ldots} = \sigma(\mathbf{Z}_{i,j,k,\ldots})\]
        </div>
        <p>
            Common activation functions include:
        </p>
        <ul>
            <li><strong>ReLU</strong>: \(\sigma(x) = \max(0, x)\)</li>
            <li><strong>Sigmoid</strong>: \(\sigma(x) = \frac{1}{1 + e^{-x}}\)</li>
            <li><strong>Tanh</strong>: \(\sigma(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\)</li>
        </ul>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>Tensor Transformations</h2>
        
        <h3>Reshaping</h3>
        <p>
            Reshaping changes the dimensions of a tensor while preserving the total number of elements. For a tensor \(\mathbf{T}\) with shape \((d_1, d_2, \ldots, d_n)\) and \(\prod_{i=1}^{n} d_i = \prod_{j=1}^{m} e_j\), we can reshape it to shape \((e_1, e_2, \ldots, e_m)\).
        </p>

        <h3>Transposition</h3>
        <p>
            Transposition permutes the dimensions of a tensor. For a 2D tensor (matrix), transposition swaps rows and columns:
        </p>
        <div class="formula">
            \[\mathbf{A}^T_{ij} = \mathbf{A}_{ji}\]
        </div>
        <p>
            For higher-order tensors, we specify the permutation of dimensions.
        </p>

        <h3>Slicing</h3>
        <p>
            Slicing extracts a subset of elements from a tensor along specified dimensions:
        </p>
        <div class="formula">
            \[\mathbf{B} = \mathbf{A}[i_1:j_1, i_2:j_2, \ldots, i_n:j_n]\]
        </div>
        <p>
            This creates a new tensor \(\mathbf{B}\) containing the elements of \(\mathbf{A}\) with indices in the ranges specified.
        </p>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>Conclusion</h2>
        <p>
            Tensors provide a powerful mathematical framework for representing and manipulating multi-dimensional data. Their algebraic properties make them ideal for implementing efficient numerical computations, especially on parallel hardware like GPUs. In the context of deep learning, tensors are the fundamental data structure for representing weights, activations, gradients, and other essential components of neural networks.
        </p>
        <p>
            Understanding the mathematical properties of tensors is crucial for developing an intuition about how neural networks process information and learn from data. As we proceed through this course, we will build upon this theoretical foundation to explore automatic differentiation, neural network architectures, and optimization methods.
        </p>
    </section>
</body>
</html>