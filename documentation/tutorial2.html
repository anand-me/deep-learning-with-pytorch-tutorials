<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2: Autograd and Automatic Differentiation - Mathematical Foundations</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        
        h1, h2, h3, h4 {
            color: #2c3e50;
            margin-top: 24px;
        }
        
        h1 {
            border-bottom: 2px solid #9b59b6;
            padding-bottom: 10px;
            font-size: 2.5em;
            text-align: center;
        }
        
        h2 {
            border-left: 5px solid #9b59b6;
            padding-left: 10px;
            background-color: #ecf0f1;
            padding: 8px 12px;
        }
        
        .author-info {
            text-align: center;
            margin-bottom: 40px;
        }
        
        .definition {
            background-color: #f4ecf7;
            border-left: 4px solid #9b59b6;
            padding: 15px;
            margin: 20px 0;
        }
        
        .note {
            background-color: #e8f8f5;
            border-left: 4px solid #1abc9c;
            padding: 15px;
            margin: 20px 0;
        }
        
        .example {
            background-color: #ebf5fb;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin: 20px 0;
        }
        
        .formula {
            background-color: #f5f5f5;
            padding: 15px;
            margin: 20px 0;
            text-align: center;
            font-family: 'Cambria Math', Georgia, serif;
            font-size: 1.1em;
        }
        
        .section-separator {
            border-top: 1px dashed #bdc3c7;
            margin: 40px 0;
        }
        
        .github-link {
            display: inline-block;
            font-style: italic;
            color: #9b59b6;
            text-decoration: none;
        }
        
        .github-link:hover {
            text-decoration: underline;
        }
        
        .concept-container {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            margin: 20px 0;
        }
        
        .concept-box {
            flex: 1 1 300px;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            background-color: white;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        
        .concept-box h3 {
            margin-top: 0;
            border-bottom: 1px solid #eee;
            padding-bottom: 8px;
        }
        
        figure {
            text-align: center;
            margin: 20px 0;
        }
        
        figcaption {
            font-style: italic;
            margin-top: 8px;
            color: #666;
        }
        
        .function-diagram {
            max-width: 100%;
            height: auto;
            margin: 20px auto;
            display: block;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            
            .concept-box {
                flex: 1 1 100%;
            }
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>Chapter 2: Autograd and Automatic Differentiation</h1>
        <div class="author-info">
            <h3>Author: Akshay Anand, PhD Candidate</h3>
            <p><strong>Florida State University (2021 - 20XX)</strong></p>
            <a href="https://github.com/anand-me" class="github-link">GitHub</a>
        </div>
    </header>

    <section>
        <h2>Introduction</h2>
        <p>
            Automatic differentiation is the backbone of modern deep learning frameworks, enabling neural networks to learn through gradient-based optimization. In this chapter, we will explore the mathematical principles behind automatic differentiation, particularly focusing on PyTorch's autograd system.
        </p>
        <p>
            By the end of this chapter, you will understand:
        </p>
        <ul>
            <li>The mathematical foundations of gradient computation</li>
            <li>Computational graphs and their role in automatic differentiation</li>
            <li>Forward and backward mode automatic differentiation</li>
            <li>How PyTorch implements automatic differentiation through autograd</li>
        </ul>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>1. Understanding Gradients</h2>
        
        <h3>1.1 Mathematical Definition of Gradients</h3>
        <p>
            The gradient of a scalar-valued function \(f: \mathbb{R}^n \rightarrow \mathbb{R}\) with respect to its input vector \(\mathbf{x} = (x_1, x_2, \ldots, x_n)\) is a vector containing all partial derivatives:
        </p>
        <div class="formula">
            \[\nabla_\mathbf{x} f(\mathbf{x}) = \begin{pmatrix}
            \frac{\partial f}{\partial x_1} \\
            \frac{\partial f}{\partial x_2} \\
            \vdots \\
            \frac{\partial f}{\partial x_n}
            \end{pmatrix}\]
        </div>
        <p>
            This gradient vector points in the direction of the steepest increase of the function \(f\).
        </p>

        <div class="definition">
            <h4>Key Properties of Gradients</h4>
            <ol>
                <li>The gradient is orthogonal to the level curves (or surfaces) of the function</li>
                <li>The magnitude of the gradient indicates the steepness of the function at that point</li>
                <li>The negative gradient points in the direction of steepest descent, which is why we use it in gradient descent optimization</li>
            </ol>
        </div>

        <h3>1.2 Gradients in Neural Networks</h3>
        <p>
            In neural networks, we're interested in computing the gradient of a loss function \(L\) with respect to the network parameters \(\theta\):
        </p>
        <div class="formula">
            \[\nabla_\theta L(\theta) = \begin{pmatrix}
            \frac{\partial L}{\partial \theta_1} \\
            \frac{\partial L}{\partial \theta_2} \\
            \vdots \\
            \frac{\partial L}{\partial \theta_n}
            \end{pmatrix}\]
        </div>
        <p>
            This gradient tells us how to adjust each parameter to decrease the loss, forming the basis of the gradient descent algorithm:
        </p>
        <div class="formula">
            \[\theta_{\text{new}} = \theta_{\text{old}} - \alpha \nabla_\theta L(\theta)\]
        </div>
        <p>
            where \(\alpha\) is the learning rate.
        </p>

        <h3>1.3 Jacobian and Hessian Matrices</h3>
        <p>
            For vector-valued functions \(\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m\), we use the Jacobian matrix to represent the first-order derivatives:
        </p>
        <div class="formula">
            \[\mathbf{J}_\mathbf{f} = \begin{pmatrix}
            \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
            \vdots & \ddots & \vdots \\
            \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
            \end{pmatrix}\]
        </div>
        <p>
            The Hessian matrix represents the second-order derivatives of a scalar function \(f: \mathbb{R}^n \rightarrow \mathbb{R}\):
        </p>
        <div class="formula">
            \[\mathbf{H}_f = \begin{pmatrix}
            \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
            \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
            \end{pmatrix}\]
        </div>
        <p>
            The Hessian matrix provides information about the local curvature of the function and is used in second-order optimization methods like Newton's method.
        </p>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>2. Computational Graphs</h2>
        
        <h3>2.1 Definition and Structure</h3>
        <p>
            A computational graph is a directed acyclic graph (DAG) that represents the computation of a mathematical expression. The nodes in this graph correspond to operations or variables, and the edges represent the flow of data between operations.
        </p>

        <div class="definition">
            <h4>Components of a Computational Graph</h4>
            <ul>
                <li><strong>Leaf Nodes</strong>: Input variables or parameters</li>
                <li><strong>Interior Nodes</strong>: Operations (addition, multiplication, activation functions, etc.)</li>
                <li><strong>Root Node</strong>: The final output of the computation</li>
                <li><strong>Edges</strong>: Data flow between operations, carrying tensors</li>
            </ul>
        </div>

        <figure>
            <div style="text-align: center;">
                <svg width="500" height="300" xmlns="http://www.w3.org/2000/svg">
                    <!-- Input nodes -->
                    <circle cx="100" cy="50" r="20" fill="#9b59b6" />
                    <text x="100" cy="55" text-anchor="middle" fill="white">x</text>
                    
                    <circle cx="100" cy="150" r="20" fill="#9b59b6" />
                    <text x="100" cy="155" text-anchor="middle" fill="white">y</text>
                    
                    <circle cx="100" cy="250" r="20" fill="#9b59b6" />
                    <text x="100" cy="255" text-anchor="middle" fill="white">z</text>
                    
                    <!-- First level operations -->
                    <circle cx="200" cy="100" r="20" fill="#3498db" />
                    <text x="200" cy="105" text-anchor="middle" fill="white">*</text>
                    
                    <circle cx="200" cy="200" r="20" fill="#3498db" />
                    <text x="200" cy="205" text-anchor="middle" fill="white">+</text>
                    
                    <!-- Second level operation -->
                    <circle cx="300" cy="150" r="20" fill="#3498db" />
                    <text x="300" cy="155" text-anchor="middle" fill="white">+</text>
                    
                    <!-- Output node -->
                    <circle cx="400" cy="150" r="20" fill="#e74c3c" />
                    <text x="400" cy="155" text-anchor="middle" fill="white">f</text>
                    
                    <!-- Edges -->
                    <line x1="120" y1="50" x2="180" y2="100" stroke="black" stroke-width="2" />
                    <line x1="120" y1="150" x2="180" y2="100" stroke="black" stroke-width="2" />
                    <line x1="120" y1="150" x2="180" y2="200" stroke="black" stroke-width="2" />
                    <line x1="120" y1="250" x2="180" y2="200" stroke="black" stroke-width="2" />
                    <line x1="220" y1="100" x2="280" y2="150" stroke="black" stroke-width="2" />
                    <line x1="220" y1="200" x2="280" y2="150" stroke="black" stroke-width="2" />
                    <line x1="320" y1="150" x2="380" y2="150" stroke="black" stroke-width="2" />
                    
                    <!-- Labels -->
                    <text x="250" y="30" text-anchor="middle" fill="#333" font-weight="bold">Computational Graph for f(x,y,z) = (x*y) + (y+z)</text>
                </svg>
            </div>
            <figcaption>Figure 1: Example of a computational graph for the function f(x,y,z) = (x*y) + (y+z)</figcaption>
        </figure>

        <h3>2.2 Forward and Backward Passes</h3>
        <p>
            Computation on a graph occurs in two phases:
        </p>
        <ol>
            <li><strong>Forward Pass</strong>: Values are computed from inputs to outputs, following the edges of the graph</li>
            <li><strong>Backward Pass</strong>: Gradients are computed from outputs to inputs, against the edges of the graph</li>
        </ol>

        <h3>2.3 Example: Building a Computational Graph</h3>
        <p>
            Let's analyze the computational graph for a simple expression: \(f(x, y) = x^2 + 2xy + y^2\), which can be rewritten as \((x + y)^2\).
        </p>

        <div class="concept-container">
            <div class="concept-box">
                <h3>Forward Pass Computation</h3>
                <p>Starting with inputs \(x = 3\) and \(y = 4\):</p>
                <ol>
                    <li>Compute \(g = x + y = 3 + 4 = 7\)</li>
                    <li>Compute \(f = g^2 = 7^2 = 49\)</li>
                </ol>
            </div>
            <div class="concept-box">
                <h3>Backward Pass Computation</h3>
                <p>Computing gradients from output to inputs:</p>
                <ol>
                    <li>\(\frac{\partial f}{\partial f} = 1\) (gradient of output with respect to itself)</li>
                    <li>\(\frac{\partial f}{\partial g} = \frac{\partial}{\partial g}(g^2) = 2g = 2 \cdot 7 = 14\)</li>
                    <li>\(\frac{\partial f}{\partial x} = \frac{\partial f}{\partial g} \cdot \frac{\partial g}{\partial x} = 14 \cdot 1 = 14\)</li>
                    <li>\(\frac{\partial f}{\partial y} = \frac{\partial f}{\partial g} \cdot \frac{\partial g}{\partial y} = 14 \cdot 1 = 14\)</li>
                </ol>
            </div>
        </div>

        <div class="note">
            <p>
                In PyTorch, the computational graph is built dynamically during the forward pass, making it flexible for models with complex control flow and variable structure.
            </p>
        </div>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>3. The Chain Rule and Backpropagation</h2>
        
        <h3>3.1 The Chain Rule</h3>
        <p>
            The chain rule is a fundamental principle in calculus that allows us to compute the derivative of composite functions. For a composition of functions \(f(g(x))\), the derivative with respect to \(x\) is:
        </p>
        <div class="formula">
            \[\frac{d}{dx}[f(g(x))] = \frac{df}{dg} \cdot \frac{dg}{dx}\]
        </div>
        <p>
            This extends to multivariate functions and partial derivatives. For a function \(f(g(x, y), h(x, y))\), the partial derivative with respect to \(x\) is:
        </p>
        <div class="formula">
            \[\frac{\partial f}{\partial x} = \frac{\partial f}{\partial g} \cdot \frac{\partial g}{\partial x} + \frac{\partial f}{\partial h} \cdot \frac{\partial h}{\partial x}\]
        </div>

        <h3>3.2 Backpropagation Algorithm</h3>
        <p>
            Backpropagation is an algorithm that efficiently applies the chain rule to compute gradients in a computational graph. It consists of these steps:
        </p>
        <ol>
            <li>Perform a forward pass to compute the output value</li>
            <li>Initialize the gradient of the output with respect to itself as 1</li>
            <li>Traverse the graph in reverse topological order (from outputs to inputs)</li>
            <li>At each node, compute the gradient of the output with respect to its inputs using the chain rule</li>
            <li>Accumulate gradients at nodes with multiple outgoing paths</li>
        </ol>

        <div class="formula">
            \[\frac{\partial L}{\partial x_i} = \sum_{j \in \text{succ}(i)} \frac{\partial L}{\partial x_j} \cdot \frac{\partial x_j}{\partial x_i}\]
        </div>
        <p>
            where \(\text{succ}(i)\) represents the set of nodes that directly follow node \(i\) in the graph.
        </p>

        <h3>3.3 Gradient Accumulation</h3>
        <p>
            A key aspect of backpropagation is gradient accumulation. When a node appears in multiple paths in the computational graph, its gradient contributions from each path must be summed:
        </p>
        <div class="formula">
            \[\frac{\partial L}{\partial x} = \sum_{i} \frac{\partial L}{\partial y_i} \cdot \frac{\partial y_i}{\partial x}\]
        </div>
        <p>
            where \(y_i\) are all the intermediate nodes that directly depend on \(x\).
        </p>

        <div class="example">
            <h4>Example: Backpropagation in a Simple Neural Network</h4>
            <p>Consider a simple neural network with one hidden layer:</p>
            <ul>
                <li>Input \(\mathbf{x}\) and weights \(\mathbf{W}_1\) and \(\mathbf{W}_2\)</li>
                <li>Hidden layer: \(\mathbf{h} = \sigma(\mathbf{W}_1 \mathbf{x})\) where \(\sigma\) is an activation function</li>
                <li>Output: \(\mathbf{y} = \mathbf{W}_2 \mathbf{h}\)</li>
                <li>Loss: \(L = \frac{1}{2}||\mathbf{y} - \mathbf{t}||^2\) where \(\mathbf{t}\) is the target</li>
            </ul>
            <p>Backpropagation computes:</p>
            <ol>
                <li>\(\frac{\partial L}{\partial \mathbf{y}} = \mathbf{y} - \mathbf{t}\)</li>
                <li>\(\frac{\partial L}{\partial \mathbf{W}_2} = \frac{\partial L}{\partial \mathbf{y}} \mathbf{h}^T\)</li>
                <li>\(\frac{\partial L}{\partial \mathbf{h}} = \mathbf{W}_2^T \frac{\partial L}{\partial \mathbf{y}}\)</li>
                <li>\(\frac{\partial L}{\partial \mathbf{W}_1} = \frac{\partial L}{\partial \mathbf{h}} \odot \sigma'(\mathbf{W}_1 \mathbf{x}) \mathbf{x}^T\)</li>
            </ol>
            <p>where \(\odot\) represents element-wise multiplication.</p>
        </div>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>4. Automatic Differentiation Systems</h2>
        
        <h3>4.1 Forward Mode vs. Backward Mode</h3>
        <p>
            Automatic differentiation comes in two primary modes:
        </p>

        <div class="concept-container">
            <div class="concept-box">
                <h3>Forward Mode</h3>
                <p>
                    Computes derivatives alongside the forward computation, propagating derivatives from inputs to outputs.
                </p>
                <ul>
                    <li>Efficient for functions with <em>many outputs and few inputs</em></li>
                    <li>Computes derivatives with respect to a single input variable at a time</li>
                    <li>Computation complexity proportional to the number of inputs</li>
                </ul>
                <p>Mathematically, forward mode computes directional derivatives in the form of:</p>
                <div style="text-align: center;">
                    \( \dot{y} = \frac{\partial f}{\partial x} \dot{x} \)
                </div>
                <p>where \(\dot{x}\) is a "seed" direction and \(\dot{y}\) is the directional derivative.</p>
            </div>
            <div class="concept-box">
                <h3>Backward Mode</h3>
                <p>
                    First computes the forward pass, then propagates derivatives backward from outputs to inputs.
                </p>
                <ul>
                    <li>Efficient for functions with <em>many inputs and few outputs</em></li>
                    <li>Computes derivatives with respect to all input variables at once</li>
                    <li>Computation complexity proportional to the number of outputs</li>
                </ul>
                <p>Mathematically, backward mode computes gradients in the form of:</p>
                <div style="text-align: center;">
                    \( \bar{x} = \bar{y}^T \frac{\partial f}{\partial x} \)
                </div>
                <p>where \(\bar{y}\) is typically initialized as 1 for scalar outputs.</p>
            </div>
        </div>

        <div class="note">
            <p>
                Neural networks typically have many parameters (inputs) and a scalar loss (output), making backward mode the preferred choice for deep learning frameworks like PyTorch.
            </p>
        </div>

        <h3>4.2 Dual Numbers for Automatic Differentiation</h3>
        <p>
            Forward mode automatic differentiation can be elegantly implemented using dual numbers, which are an extension of real numbers with an infinitesimal component.
        </p>
        <p>
            A dual number has the form \(a + b\varepsilon\) where \(a, b\) are real numbers and \(\varepsilon\) is an infinitesimal with the property \(\varepsilon^2 = 0\).
        </p>
        <p>
            When a function \(f(x)\) is evaluated with a dual number \(x + \dot{x}\varepsilon\), the result is \(f(x) + f'(x)\dot{x}\varepsilon\), which gives us both the function value and its derivative.
        </p>

        <div class="formula">
            \[f(x + \dot{x}\varepsilon) = f(x) + f'(x)\dot{x}\varepsilon\]
        </div>

        <h3>4.3 Reverse Accumulation for Backward Mode</h3>
        <p>
            Backward mode uses reverse accumulation, which:
        </p>
        <ol>
            <li>Records operations during the forward pass in a "tape"</li>
            <li>Computes the gradient of the output with respect to the final result (usually 1 for scalar outputs)</li>
            <li>Traverses the tape in reverse, computing and accumulating gradients</li>
        </ol>
        <p>
            For each operation \(y = f(x)\) on the tape, we apply:
        </p>
        <div class="formula">
            \[\bar{x} += \bar{y} \cdot \frac{\partial f}{\partial x}\]
        </div>
        <p>
            where \(\bar{y}\) is the gradient of the final output with respect to \(y\), and \(\bar{x}\) accumulates the gradient of the final output with respect to \(x\).
        </p>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>5. PyTorch's Autograd System</h2>
        
        <h3>5.1 Dynamic Computational Graphs</h3>
        <p>
            PyTorch uses a dynamic computational graph system called autograd:
        </p>
        <ul>
            <li>Graphs are built on-the-fly during the forward computation</li>
            <li>Every operation creates new nodes in the graph</li>
            <li>Tensors that require gradients track the operations performed on them</li>
            <li>The graph is discarded after each backward pass (unless <code>retain_graph=True</code>)</li>
        </ul>

        <h3>5.2 Mathematical Operations in Autograd</h3>
        <p>
            When an operation involving tensors is performed, PyTorch:
        </p>
        <ol>
            <li>Executes the forward computation</li>
            <li>Creates a <code>Function</code> object that knows how to compute the gradients of the operation</li>
            <li>Connects the input and output tensors to this function in the graph</li>
            <li>Sets the <code>grad_fn</code> attribute of the output tensor to this function</li>
        </ol>

        <div class="formula">
            \[\text{For } y = f(x): \quad \frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}\]
        </div>

        <p>
            Each operation in PyTorch defines both a forward function and a backward function that computes the necessary gradients.
        </p>

        <h3>5.3 Gradient Management</h3>
        <p>
            PyTorch provides several mechanisms for managing gradients:
        </p>
        <ul>
            <li><strong>requires_grad</strong>: Flag to enable gradient tracking for a tensor</li>
            <li><strong>no_grad</strong>: Context manager to disable gradient computation temporarily</li>
            <li><strong>detach</strong>: Method to create a new tensor without gradient history</li>
            <li><strong>grad</strong>: Attribute to access accumulated gradients</li>
            <li><strong>zero_grad</strong>: Method to reset accumulated gradients to zero</li>
        </ul>

        <div class="definition">
            <h4>Key Autograd Components</h4>
            <ul>
                <li><strong>Tensor</strong>: Stores data and references to its creator</li>
                <li><strong>Function</strong>: Records operation history and computes gradients</li>
                <li><strong>Engine</strong>: Orchestrates the backward pass and gradient accumulation</li>
            </ul>
        </div>

        <h3>5.4 Higher-Order Derivatives</h3>
        <p>
            PyTorch supports higher-order derivatives (derivatives of derivatives) by treating gradient computations themselves as differentiable operations:
        </p>
        <div class="formula">
            \[\frac{\partial^2 L}{\partial x^2} = \frac{\partial}{\partial x}\left(\frac{\partial L}{\partial x}\right)\]
        </div>
        <p>
            This enables advanced optimization techniques like Newton's method and analysis of curvature.
        </p>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>Conclusion</h2>
        <p>
            Automatic differentiation, particularly as implemented in PyTorch's autograd system, is a cornerstone of modern deep learning. It combines the efficiency of symbolic differentiation with the flexibility of numerical approaches, enabling the training of complex neural networks through gradient-based optimization.
        </p>
        <p>
            The ability to compute gradients automatically through arbitrary computational graphs has revolutionized deep learning research and application, making it possible to experiment with novel architectures and loss functions without manually deriving gradients.
        </p>
        <p>
            In the next chapter, we will build upon this foundation to explore neural network architectures and how they leverage the automatic differentiation capabilities provided by PyTorch.
        </p>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>References and Further Reading</h2>
        <ol>
            <li>Baydin, A.G., Pearlmutter, B.A., Radul, A.A. and Siskind, J.M., 2018. Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research, 18(1), pp.5595-5637.</li>
            <li>Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L. and Lerer, A., 2017. Automatic differentiation in