<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 5: Saving and Loading Models in PyTorch</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        
        h1, h2, h3, h4 {
            color: #2c3e50;
            margin-top: 24px;
        }
        
        h1 {
            border-bottom: 2px solid #8e44ad;
            padding-bottom: 10px;
            font-size: 2.5em;
            text-align: center;
        }
        
        h2 {
            border-left: 5px solid #8e44ad;
            padding-left: 10px;
            background-color: #ecf0f1;
            padding: 8px 12px;
        }
        
        .author-info {
            text-align: center;
            margin-bottom: 40px;
        }
        
        .definition {
            background-color: #f5eef8;
            border-left: 4px solid #8e44ad;
            padding: 15px;
            margin: 20px 0;
        }
        
        .note {
            background-color: #e8f8f5;
            border-left: 4px solid #1abc9c;
            padding: 15px;
            margin: 20px 0;
        }
        
        .example {
            background-color: #ebf5fb;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin: 20px 0;
        }
        
        .formula {
            background-color: #f5f5f5;
            padding: 15px;
            margin: 20px 0;
            text-align: center;
            font-family: 'Cambria Math', Georgia, serif;
            font-size: 1.1em;
        }
        
        .section-separator {
            border-top: 1px dashed #bdc3c7;
            margin: 40px 0;
        }
        
        .github-link {
            display: inline-block;
            font-style: italic;
            color: #8e44ad;
            text-decoration: none;
        }
        
        .github-link:hover {
            text-decoration: underline;
        }
        
        .concept-container {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            margin: 20px 0;
        }
        
        .concept-box {
            flex: 1 1 300px;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            background-color: white;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        
        .concept-box h3 {
            margin-top: 0;
            border-bottom: 1px solid #eee;
            padding-bottom: 8px;
        }
        
        figure {
            text-align: center;
            margin: 20px 0;
        }
        
        figcaption {
            font-style: italic;
            margin-top: 8px;
            color: #666;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        table, th, td {
            border: 1px solid #ddd;
        }
        
        th {
            background-color: #8e44ad;
            color: white;
            text-align: left;
            padding: 10px;
        }
        
        td {
            padding: 10px;
        }
        
        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        
        .reference {
            padding-left: 20px;
            text-indent: -20px;
            margin-bottom: 10px;
        }
        
        .code-block {
            background-color: #f5f5f5;
            padding: 15px;
            border-radius: 5px;
            font-family: 'Courier New', Courier, monospace;
            white-space: pre-wrap;
            margin: 15px 0;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            
            .concept-box {
                flex: 1 1 100%;
            }
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>Chapter 5: Saving and Loading Models in PyTorch</h1>
        <div class="author-info">
            <h3>Author: Akshay Anand, PhD Candidate</h3>
            <p><strong>Florida State University (2021 - 20XX)</strong></p>
            <a href="https://github.com/anand-me" class="github-link">GitHub</a>
        </div>
    </header>

    <section>
        <h2>Introduction</h2>
        <p>
            After investing time and computational resources in training neural networks, it's crucial to save the trained models for later use and deployment. This chapter explores the theoretical and practical aspects of model serialization, loading, and deployment in PyTorch.
        </p>
        <p>
            By the end of this chapter, you will understand:
        </p>
        <ul>
            <li>Different methods for saving and loading PyTorch models</li>
            <li>How to handle model checkpointing during training</li>
            <li>Techniques for deploying models in production environments</li>
            <li>Approaches for optimizing models for inference</li>
        </ul>
    </section>

    <div class="section-separator"></div>

    <section>
        <h2>1. Model Serialization in PyTorch</h2>
        
        <h3>1.1 Understanding Model Serialization</h3>
        <p>
            Model serialization is the process of converting a model's state (parameters, architecture, etc.) into a format that can be saved to disk and later reconstructed <sup>[1]</sup>. In PyTorch, serialization is primarily handled through the pickle module, which enables Python objects to be serialized and deserialized.
        </p>
        <p>
            The key components of a PyTorch model that may need to be serialized include:
        </p>
        <ol>
            <li>
                <strong>Model Architecture</strong>: The structure and connections between layers
            </li>
            <li>
                <strong>Model Parameters</strong>: The weights and biases of the model
            </li>
            <li>
                <strong>Optimizer State</strong>: Information about the optimizer's state (momentum, adaptive learning rates, etc.)
            </li>
            <li>
                <strong>Training Metadata</strong>: Epoch number, loss history, learning rate schedules, etc.
            </li>
        </ol>

        <h3>1.2 Saving and Loading Model Parameters</h3>
        <p>
            The most common and efficient approach for saving PyTorch models is to save only the model parameters (weights and biases) <sup>[1]</sup>. This method offers several advantages: smaller file size, flexibility in model architecture changes, and compatibility across different environments.
        </p>

        <div class="definition">
            <h4>State Dictionary</h4>
            <p>
                In PyTorch, model parameters are stored in a Python dictionary called the state dictionary (<code>state_dict</code>). This dictionary maps each layer's name to its parameter tensors.
            </p>
        </div>

        <div class="concept-container">
            <div class="concept-box">
                <h3>Saving the State Dictionary</h3>
                <p>
                    To save only the model parameters, use <code>torch.save()</code> with the model's state dictionary:
                </p>
                <div class="code-block">
torch.save(model.state_dict(), 'model_weights.pth')
                </div>
                <p>
                    This approach has several benefits:
                </p>
                <ul>
                    <li>Smaller file size</li>
                    <li>Flexibility to load into different model architectures</li>
                    <li>Better compatibility across PyTorch versions</li>
                    <li>Recommended for production environments</li>
                </ul>
            </div>
            <div class="concept-box">
                <h3>Loading the State Dictionary</h3>
                <p>
                    To load the saved parameters into a model, first initialize a model instance with the correct architecture, then load the state dictionary:
                </p>
                <div class="code-block">
model = YourModelClass()
model.load_state_dict(torch.load('model_weights.pth'))
model.eval()  # Set to evaluation mode
                </div>
                <p>
                    The <code>load_state_dict()</code> method updates the model's parameters with those loaded from the file. The <code>eval()</code> call is essential when using the model for inference, as it affects layers like Dropout and BatchNorm.
                </p>
            </div>
        </div>

        <div class="note">
            <p>
                When loading a state dictionary, the model architecture must match the saved parameters. Any mismatch in layer names or dimensions will result in an error <sup>[2]</sup>.
            </p>
        </div>

        <h3>1.3 Saving and Loading the Entire Model</h3>
        <p>
            While saving the state dictionary is the recommended approach, PyTorch also allows saving the entire model, including its architecture <sup>[3]</sup>. This method is simpler but less flexible.
        </p>

        <div class="concept-container">
            <div class="concept-box">
                <h3>Saving the Entire Model</h3>
                <p>
                    To save the complete model, pass the model object directly to <code>torch.save()</code>:
                </p>
                <div class="code-block">
torch.save(model, 'complete_model.pth')
                </div>
                <p>
                    This approach:
                </p>
                <ul>
                    <li>Saves the model architecture along with parameters</li>
                    <li>Creates larger file sizes</li>
                    <li>Is more convenient for quick prototyping</li>
                    <li>May cause compatibility issues across different PyTorch versions</li>
                </ul>
            </div>
            <div class="concept-box">
                <h3>Loading the Entire Model</h3>
                <p>
                    To load the complete model:
                </p>
                <div class="code-block">
model = torch.load('complete_model.pth')
model.eval()  # Set to evaluation mode
                </div>
                <p>
                    This method reconstructs the entire model object, including its architecture and parameters.
                </p>
            </div>
        </div>

        <div class="note">
            <p>
                In PyTorch 2.0+, additional steps are required to safely load full models due to improved security measures. The <code>torch.serialization.safe_globals</code> context manager or explicitly setting <code>weights_only=False</code> might be necessary <sup>[4]</sup>.
            </p>
        </div>

        <h3>1.4 Saving Training Checkpoints</h3>
        <p>
            For long training processes, it's essential to save checkpoints that enable resuming training from where it left off. A comprehensive checkpoint includes the model state, optimizer state, and training metadata <sup>[5]</sup>.
        </p>

        <div class="example">
            <h4>Comprehensive Checkpoint Format</h4>
            <div class="code-block">
checkpoint = {
    'epoch': current_epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
    'loss': current_loss,
    'best_metric': best_validation_metric,
    'training_config': {
        'batch_size': batch_size,
        'learning_rate': initial_lr,
        'dropout_rate': dropout_rate
    }
}

torch.save(checkpoint, f'checkpoint_epoch_{current_epoch}.pth')
            </div>
        </div>

        <p>
            To resume training from a checkpoint:
        </p>

        <div class="code-block">
# Load checkpoint
checkpoint = torch.load('checkpoint_epoch_N.pth')

# Restore model state
model = YourModelClass()
model.load_state_dict(checkpoint['model_state_dict'])

# Restore optimizer state
optimizer = torch.optim.Adam(model.parameters())
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

# Restore scheduler if used
if checkpoint['scheduler_state_dict']:
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10)
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

# Restore training metadata
start_epoch = checkpoint['epoch'] + 1
best_metric = checkpoint['best_metric']

# Continue training from this point
        </div>

        <div class="note">
            <p>
                Checkpointing best practices <sup>[6]</sup>:
            </p>
            <ul>
                <li>Save at regular intervals (e.g., every epoch or N iterations)</li>
                <li>Keep multiple checkpoints (e.g., last K epochs)</li>
                <li>Save separate checkpoints for best-performing models based on validation metrics</li>
                <li>Include version information for reproducibility</li>
            </ul>
        </div>

        <h3>1.5 Device Considerations</h3>
        <p>
            When loading models across different devices (CPU vs. GPU), additional steps are needed to ensure compatibility <sup>[7]</sup>.
        </p>

        <div class="concept-container">
            <div class="concept-box">
                <h3>Saving Models from GPU</h3>
                <p>
                    Models trained on GPU have their parameters stored on CUDA tensors. You can either:
                </p>
                <div class="code-block">
# Option 1: Move to CPU before saving
torch.save(model.cpu().state_dict(), 'model_cpu.pth')

# Option 2: Save directly from GPU
torch.save(model.state_dict(), 'model_gpu.pth')
                </div>
            </div>
            <div class="concept-box">
                <h3>Loading Models to Specific Devices</h3>
                <p>
                    When loading a model, you can specify the target device:
                </p>
                <div class="code-block">
# Load to CPU
model.load_state_dict(torch.load('model.pth', map_location='cpu'))

# Load to specific GPU
model.load_state_dict(torch.load('model.pth', map_location='cuda:0'))

# Load to current device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.load_state_dict(torch.load('model.pth', map_location=device))
                </div>
            </div>
        </div>

        <div class="section-separator"></div>

        <section>
            <h2>2. Loading Pretrained Models</h2>
            
            <h3>2.1 Using Models from Model Hubs</h3>
            <p>
                PyTorch offers convenient access to pretrained models through various model hubs, enabling rapid development through transfer learning <sup>[8]</sup>.
            </p>
            
            <div class="concept-container">
                <div class="concept-box">
                    <h3>TorchVision Models</h3>
                    <p>
                        TorchVision provides pretrained models for computer vision tasks:
                    </p>
                    <div class="code-block">
from torchvision import models

# Load a pretrained ResNet-50
resnet = models.resnet50(pretrained=True)

# For newer PyTorch versions (>= 1.13)
resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
                    </div>
                    <p>
                        Available architectures include ResNet, VGG, DenseNet, Inception, EfficientNet, and others.
                    </p>
                </div>
                <div class="concept-box">
                    <h3>Hugging Face Models</h3>
                    <p>
                        Hugging Face's Transformers library provides pretrained models for NLP tasks <sup>[9]</sup>:
                    </p>
                    <div class="code-block">
from transformers import BertModel, BertTokenizer

# Load a pretrained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
                    </div>
                    <p>
                        Models for text, vision, audio, and multimodal tasks are available.
                    </p>
                </div>
            </div>
            
            <div class="concept-container">
                <div class="concept-box">
                    <h3>PyTorch Hub</h3>
                    <p>
                        PyTorch Hub offers a central repository for sharing and loading pretrained models <sup>[10]</sup>:
                    </p>
                    <div class="code-block">
import torch

# Load YOLO model from Ultralytics
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

# Load SSD model
model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')
                    </div>
                </div>
                <div class="concept-box">
                    <h3>Using Custom Pretrained Models</h3>
                    <p>
                        For custom models shared within teams or communities:
                    </p>
                    <div class="code-block">
# Define your model architecture
model = YourCustomModel()

# Load pretrained weights
state_dict = torch.load('pretrained_weights.pth')

# Handle potential key mismatches
from collections import OrderedDict
new_state_dict = OrderedDict()
for k, v in state_dict.items():
    if k.startswith('module.'):  # Remove 'module.' prefix if needed
        name = k[7:]  # Remove 'module.'
    else:
        name = k
    new_state_dict[name] = v

# Load the weights
model.load_state_dict(new_state_dict)
                    </div>
                </div>
            </div>

            <div class="note">
                <p>
                    When using pretrained models, consider these factors <sup>[11]</sup>:
                </p>
                <ul>
                    <li>Input preprocessing requirements (normalization, resizing, etc.)</li>
                    <li>Output format and interpretation</li>
                    <li>Memory and computational requirements</li>
                    <li>Ethical considerations (dataset biases, etc.)</li>
                </ul>
            </div>

            <h3>2.2 Transfer Learning with Pretrained Models</h3>
            <p>
                Transfer learning leverages knowledge from pretrained models for new tasks, significantly reducing training time and data requirements <sup>[12]</sup>.
            </p>

            <ol>
                <li>
                    <strong>Feature Extraction</strong>: Use the pretrained model as a fixed feature extractor.
                    <div class="code-block">
# Load a pretrained model
pretrained_model = models.resnet50(pretrained=True)

# Freeze all parameters
for param in pretrained_model.parameters():
    param.requires_grad = False

# Replace the final layer
num_features = pretrained_model.fc.in_features
pretrained_model.fc = nn.Linear(num_features, num_classes)  # Only these parameters will be updated
                    </div>
                </li>
                <li>
                    <strong>Fine-Tuning</strong>: Adapt all or part of the pretrained model.
                    <div class="code-block">
# Load a pretrained model
pretrained_model = models.resnet50(pretrained=True)

# Fine-tune with different learning rates
optimizer = torch.optim.SGD([
    {'params': pretrained_model.conv1.parameters(), 'lr': 1e-5},  # Early layers: lower learning rate
    {'params': pretrained_model.layer4.parameters(), 'lr': 1e-4},  # Later layers: higher learning rate
    {'params': pretrained_model.fc.parameters(), 'lr': 1e-3}      # New layers: highest learning rate
], momentum=0.9)
                    </div>
                </li>
            </ol>

            <div class="example">
                <h4>Transfer Learning Strategies Based on Data Size and Similarity</h4>
                <table>
                    <tr>
                        <th>Target Dataset Size</th>
                        <th>Similarity to Source</th>
                        <th>Recommended Strategy</th>
                    </tr>
                    <tr>
                        <td>Small</td>
                        <td>High</td>
                        <td>Feature extraction (freeze all but last layers)</td>
                    </tr>
                    <tr>
                        <td>Small</td>
                        <td>Low</td>
                        <td>Feature extraction + extensive data augmentation</td>
                    </tr>
                    <tr>
                        <td>Large</td>
                        <td>High</td>
                        <td>Fine-tuning with small learning rate</td>
                    </tr>
                    <tr>
                        <td>Large</td>
                        <td>Low</td>
                        <td>Fine-tuning with progressive unfreezing</td>
                    </tr>
                </table>
            </div>
        </section>

        <div class="section-separator"></div>

        <section>
            <h2>3. Model Deployment</h2>
            
            <h3>3.1 Export to TorchScript</h3>
            <p>
                TorchScript is a way to create serializable and optimizable models from PyTorch code <sup>[13]</sup>. It allows PyTorch models to be used in production environments without a Python dependency.
            </p>

            <div class="concept-container">
                <div class="concept-box">
                    <h3>Tracing</h3>
                    <p>
                        Tracing captures the model's execution when run on example inputs:
                    </p>
                    <div class="code-block">
# Create an example input
example_input = torch.rand(1, 3, 224, 224)

# Trace the model
traced_model = torch.jit.trace(model, example_input)

# Save the traced model
traced_model.save("traced_model.pt")
                    </div>
                    <p>
                        Tracing works well for models without control flow dependencies.
                    </p>
                </div>
                <div class="concept-box">
                    <h3>Scripting</h3>
                    <p>
                        Scripting directly analyzes the model code to handle control flow:
                    </p>
                    <div class="code-block">
# Script the model
scripted_model = torch.jit.script(model)

# Save the scripted model
scripted_model.save("scripted_model.pt")
                    </div>
                    <p>
                        Scripting works better for models with conditional logic, loops, or complex operations.
                    </p>
                </div>
            </div>

            <p>
                Loading a TorchScript model:
            </p>
            <div class="code-block">
# Load in Python
loaded_model = torch.jit.load("model.pt")
result = loaded_model(input_data)

# Can also be loaded in C++, Java, etc.
            </div>

            <div class="note">
                <p>
                    TorchScript offers several advantages for deployment <sup>[14]</sup>:
                </p>
                <ul>
                    <li>Language independence (can be run from C++, Java, etc.)</li>
                    <li>Runtime performance optimization</li>
                    <li>Smaller deployment size</li>
                    <li>No Python interpreter dependency</li>
                </ul>
            </div>

            <h3>3.2 Quantization for Efficiency</h3>
            <p>
                Quantization reduces model precision to improve inference speed and memory usage <sup>[15]</sup>. This involves converting floating-point numbers (typically float32) to lower precision (int8, float16).
            </p>

            <div class="concept-container">
                <div class="concept-box">
                    <h3>Post-Training Quantization</h3>
                    <p>
                        Applied after training is complete:
                    </p>
                    <div class="code-block">
# Dynamic quantization (weights quantized during loading)
quantized_model = torch.quantization.quantize_dynamic(
    model, {nn.Linear, nn.LSTM}, dtype=torch.qint8
)

# Static quantization (requires calibration)
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)
# Calibrate with sample data...
torch.quantization.convert(model, inplace=True)
                    </div>
                </div>
                <div class="concept-box">
                    <h3>Quantization-Aware Training</h3>
                    <p>
                        Incorporates quantization effects during training:
                    </p>
                    <div class="code-block">
# Prepare the model for QAT
model_fp32 = create_model()
model_fp32.train()

model = torch.quantization.QuantWrapper(model_fp32)
model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
torch.quantization.prepare_qat(model, inplace=True)

# Train the model...

# Convert to quantized model
torch.quantization.convert(model, inplace=True)
                    </div>
                </div>
            </div>

            <div class="example">
                <h4>Benefits of Quantization</h4>
                <p>
                    Quantization can significantly reduce model size and improve inference speed with minimal accuracy degradation <sup>[16]</sup>:
                </p>
                <table>
                    <tr>
                        <th>Model Format</th>
                        <th>Memory Usage</th>
                        <th>Relative Inference Speed</th>
                        <th>Accuracy Impact</th>
                    </tr>
                    <tr>
                        <td>Float32 (original)</td>
                        <td>100%</td>
                        <td>1.0x</td>
                        <td>Baseline</td>
                    </tr>
                    <tr>
                        <td>Float16</td>
                        <td>≈50%</td>
                        <td>≈1.5-2.0x</td>
                        <td>Minimal (< 0.5%)</td>
                    </tr>
                    <tr>
                        <td>Int8 (dynamic)</td>
                        <td>≈25%</td>
                        <td>≈2.0-4.0x</td>
                        <td>Small (< 2%)</td>
                    </tr>
                    <tr>
                        <td>Int8 (static)</td>
                        <td>≈25%</td>
                        <td>≈3.0-4.0x</td>
                        <td>Moderate (< 5%)</td>
                    </tr>
                </table>
            </div>

            <h3>3.3 ONNX Format</h3>
            <p>
                The Open Neural Network Exchange (ONNX) format provides a standardized representation for deep learning models, enabling interoperability between frameworks <sup>[17]</sup>.
            </p>

            <div class="code-block">
# Export to ONNX format
import torch.onnx

# Create example input with the correct shape
dummy_input = torch.randn(1, 3, 224, 224)

# Export the model
torch.onnx.export(
    model,               # model being run
    dummy_input,         # model input
    "model.onnx",        # output file
    export_params=True,  # store the trained weights
    opset_version=11,    # the ONNX version to export to
    do_constant_folding=True,  # optimization
    input_names=['input'],     # input names
    output_names=['output'],   # output names
    dynamic_axes={'input': {0: 'batch_size'},  # variable length axes
                'output': {0: 'batch_size'}}
)
            </div>

            <p>
                ONNX models can be run using ONNX Runtime, TensorRT, OpenVINO, and other inference frameworks <sup>[18]</sup>:
            </p>
            <div class="code-block">
import onnxruntime as ort

# Create an ONNX inference session
ort_session = ort.InferenceSession("model.onnx")

# Run inference
ort_inputs = {ort_session.get_inputs()[0].name: input_data.numpy()}
ort_outputs = ort_session.run(None, ort_inputs)
            </div>

            <div class="note">
                <p>
                    Benefits of ONNX for deployment <sup>[19]</sup>:
                </p>
                <ul>
                    <li>Framework independence (PyTorch, TensorFlow, etc.)</li>
                    <li>Hardware optimization through specialized runtimes</li>
                    <li>Model visualization and optimization tools</li>
                    <li>Deployment on various platforms (cloud, edge, mobile)</li>
                </ul>
            </div>

            <h3>3.4 Mobile Deployment</h3>
            <p>
                For deploying PyTorch models on mobile devices, PyTorch Mobile provides an end-to-end workflow <sup>[20]</sup>.
            </p>

            <div class="concept-container">
                <div class="concept-box">
                    <h3>Model Optimization for Mobile</h3>
                    <p>
                        Optimize the model for mobile deployment:
                    </p>
                    <div class="code-block">
import torch.utils.mobile_optimizer as mobile_optimizer

# Export to TorchScript
scripted_model = torch.jit.script(model)

# Optimize for mobile
optimized_model = mobile_optimizer.optimize_for_mobile(scripted_model)
optimized_model.save("mobile_model.pt")
  </div>
                </div>
				 <div class="concept-box">
                    <h3>Mobile Integration</h3>
                    <p>
                        Add PyTorch Mobile to your app:
                    </p>
                    <p><strong>Android (build.gradle)</strong></p>
                    <div class="code-block">
dependencies {
    implementation 'org.pytorch:pytorch_android:1.10.0'
    implementation 'org.pytorch:pytorch_android_torchvision:1.10.0'
}
                    </div>
                    <p><strong>iOS (Podfile)</strong></p>
                    <div class="code-block">
target 'YourApp' do
  pod 'LibTorch', '~> 1.10.0'
end
                    </div>
                </div>
            </div>

            <div class="example">
                <h4>Mobile-Optimized Models</h4>
                <p>
                    For efficient mobile deployment, consider these specific architectures <sup>[21]</sup>:
                </p>
                <ul>
                    <li><strong>MobileNet</strong>: Lightweight CNN architecture designed for mobile and embedded vision applications</li>
                    <li><strong>EfficientNet</strong>: Scalable architecture with excellent accuracy-efficiency trade-off</li>
                    <li><strong>MobileViT</strong>: Combines benefits of CNNs and Vision Transformers for mobile devices</li>
                    <li><strong>SqueezeNet</strong>: Very small architecture with AlexNet-level accuracy</li>
                </ul>
            </div>
        </section>

        <div class="section-separator"></div>

        <section>
            <h2>4. Deployment Considerations</h2>
            
            <h3>4.1 Inference Optimization</h3>
            <p>
                Optimizing inference performance is crucial for production deployment <sup>[22]</sup>:
            </p>
            <ol>
                <li>
                    <strong>Batch Processing</strong>: Process multiple inputs at once where possible.
                    <div class="code-block">
# Instead of multiple single inferences
with torch.no_grad():
    result_batch = model(input_batch)  # More efficient
                    </div>
                </li>
                <li>
                    <strong>Disabling Gradients</strong>: Use <code>torch.no_grad()</code> for inference.
                    <div class="code-block">
with torch.no_grad():
    outputs = model(inputs)
                    </div>
                </li>
                <li>
                    <strong>Fusion and Optimization</strong>: Use model optimization tools.
                    <div class="code-block">
from torch.utils.mobile_optimizer import optimize_for_mobile
optimized_model = optimize_for_mobile(scripted_model)
                    </div>
                </li>
                <li>
                    <strong>Appropriate Precision</strong>: Choose the right numerical precision for your use case.
                    <div class="code-block">
# For GPU inference, half precision can double throughput
if torch.cuda.is_available():
    model = model.half()  # Convert to float16
    inputs = inputs.half()
                    </div>
                </li>
            </ol>

            <h3>4.2 Serving with TorchServe</h3>
            <p>
                TorchServe is a flexible and easy-to-use tool for serving PyTorch models <sup>[23]</sup>.
            </p>

            <div class="concept-container">
                <div class="concept-box">
                    <h3>Setup and Archiving</h3>
                    <p>
                        Package your model for TorchServe:
                    </p>
                    <div class="code-block">
# Package model as .mar file
torch-model-archiver --model-name my_model \
                     --version 1.0 \
                     --model-file model.py \
                     --serialized-file model.pth \
                     --handler image_classifier \
                     --export-path model-store
                    </div>
                </div>
                <div class="concept-box">
                    <h3>Serving the Model</h3>
                    <p>
                        Start the model server and make predictions:
                    </p>
                    <div class="code-block">
# Start TorchServe
torchserve --start --model-store model-store \
           --models my_model=my_model.mar

# Make a prediction
curl http://127.0.0.1:8080/predictions/my_model -T image.jpg
                    </div>
                </div>
            </div>

            <div class="note">
                <p>
                    TorchServe features <sup>[24]</sup>:
                </p>
                <ul>
                    <li>Model versioning and A/B testing</li>
                    <li>Metrics collection</li>
                    <li>RESTful endpoints</li>
                    <li>Batching and scaling capabilities</li>
                    <li>Custom handlers for pre/post-processing</li>
                </ul>
            </div>

            <h3>4.3 Cloud Deployment</h3>
            <p>
                Deploying PyTorch models on cloud platforms provides scalability and managed infrastructure <sup>[25]</sup>.
            </p>

            <div class="concept-container">
                <div class="concept-box">
                    <h3>AWS SageMaker</h3>
                    <p>
                        Deploy PyTorch models on AWS:
                    </p>
                    <div class="code-block">
# Example Lambda function with SageMaker runtime
import boto3

def lambda_handler(event, context):
    # Get the SageMaker runtime client
    runtime = boto3.client('sagemaker-runtime')
    
    # Make a prediction
    response = runtime.invoke_endpoint(
        EndpointName='pytorch-endpoint',
        ContentType='application/json',
        Body=json.dumps(event)
    )
    
    # Parse and return the prediction
    result = json.loads(response['Body'].read().decode())
    return result
                    </div>
                </div>
                <div class="concept-box">
                    <h3>Google Cloud AI Platform</h3>
                    <p>
                        Deploy PyTorch models on GCP:
                    </p>
                    <div class="code-block">
from google.cloud import aiplatform

# Initialize Vertex AI
aiplatform.init(project='your-project')

# Upload and deploy model
model = aiplatform.Model.upload(
    display_name='pytorch-model',
    artifact_uri='gs://your-bucket/model/',
    serving_container_image_uri='pytorch-serving-container'
)

# Deploy the model
endpoint = model.deploy(
    machine_type='n1-standard-4',
    min_replica_count=1,
    max_replica_count=5
)
                    </div>
                </div>
            </div>

            <div class="example">
                <h4>Cloud Deployment Comparison</h4>
                <table>
                    <tr>
                        <th>Platform</th>
                        <th>PyTorch Support</th>
                        <th>Key Features</th>
                    </tr>
                    <tr>
                        <td>AWS SageMaker</td>
                        <td>Native</td>
                        <td>Integrated MLOps, automatic scaling, distributed training</td>
                    </tr>
                    <tr>
                        <td>GCP Vertex AI</td>
                        <td>Native</td>
                        <td>AutoML integration, unified platform, custom containers</td>
                    </tr>
                    <tr>
                        <td>Azure ML</td>
                        <td>Native</td>
                        <td>Designer UI, hyperparameter tuning, model interpretability</td>
                    </tr>
                    <tr>
                        <td>Hugging Face Inference API</td>
                        <td>Specialized</td>
                        <td>Transformer models, simple API, inference endpoints</td>
                    </tr>
                </table>
            </div>

            <h3>4.4 Monitoring and Maintenance</h3>
            <p>
                Maintaining deployed models requires monitoring performance and updating when necessary <sup>[26]</sup>:
            </p>
            <ol>
                <li>
                    <strong>Monitoring</strong>: Track inference performance, resource usage, and prediction quality.
                    <ul>
                        <li>Inference latency and throughput</li>
                        <li>Resource utilization (CPU, GPU, memory)</li>
                        <li>Data drift detection</li>
                        <li>Error rates and outliers</li>
                    </ul>
                </li>
                <li>
                    <strong>Model Versioning</strong>: Implement a versioning system for model updates.
                    <div class="code-block">
# Example version tagging in model filename
model_version = "1.2.3"  # major.minor.patch
model_path = f"model_v{model_version}.pth"
torch.save(model.state_dict(), model_path)
                    </div>
                </li>
                <li>
                    <strong>A/B Testing</strong>: Compare performance of model variants.
                    <ul>
                        <li>Deploy multiple versions concurrently</li>
                        <li>Route a percentage of traffic to each variant</li>
                        <li>Evaluate performance metrics</li>
                        <li>Gradually shift traffic to the best-performing variant</li>
                    </ul>
                </li>
                <li>
                    <strong>Retraining Strategy</strong>: Determine when and how to retrain models.
                    <ul>
                        <li>Schedule-based: Regular intervals (weekly, monthly)</li>
                        <li>Performance-based: When metrics degrade beyond a threshold</li>
                        <li>Data-based: When input distribution shifts significantly</li>
                    </ul>
                </li>
            </ol>
        </section>

        <div class="section-separator"></div>

        <section>
            <h2>Conclusion</h2>
            <p>
                Saving, loading, and deploying PyTorch models effectively is a critical skill for transitioning from experimentation to production. This chapter covered the essential techniques for model serialization, common deployment strategies, and optimization methods for inference.
            </p>
            <p>
                Key takeaways include:
            </p>
            <ul>
                <li>Prefer saving model state dictionaries for flexibility and compatibility</li>
                <li>Implement comprehensive checkpointing for long training processes</li>
                <li>Understand device considerations when working across CPU and GPU environments</li>
                <li>Leverage transfer learning from pretrained models to accelerate development</li>
                <li>Choose the appropriate deployment format (TorchScript, ONNX) based on your production requirements</li>
                <li>Optimize inference performance through quantization and other techniques</li>
                <li>Implement proper monitoring and maintenance for deployed models</li>
            </ul>
            <p>
                By following these practices, you can effectively transition your PyTorch models from research and development to robust, scalable production deployments.
            </p>
        </section>

        <div class="section-separator"></div>

        <section>
            <h2>References</h2>
            <ol>
                <li class="reference"> PyTorch Documentation. (2023). Saving and Loading Models. <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">https://pytorch.org/tutorials/beginner/saving_loading_models.html</a></li>
                <li class="reference"> Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ... & Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32.</li>
                <li class="reference"> Howard, J., & Gugger, S. (2020). Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD. O'Reilly Media.</li>
                <li class="reference"> PyTorch Documentation. (2023). TORCH.SERIALIZATION. <a href="https://pytorch.org/docs/stable/torch.html#torch.load">https://pytorch.org/docs/stable/torch.html#torch.load</a></li>
                <li class="reference"> Grinberg, L., Ozbulak, U., Anoosheh, A., Walker, J., Lovén, L., Buddareddygari, R., ... & Hafner, D. (2022). PyTorch Lightning Documentation: Saving and Loading Checkpoints. <a href="https://pytorch-lightning.readthedocs.io/en/stable/common/checkpointing.html">https://pytorch-lightning.readthedocs.io/en/stable/common/checkpointing.html</a></li>
                <li class="reference"> Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., ... & Varoquaux, G. (2013). API design for machine learning software: experiences from the scikit-learn project. arXiv preprint arXiv:1309.0238.</li>
                <li class="reference"> PyTorch Documentation. (2023). CUDA Semantics. <a href="https://pytorch.org/docs/stable/notes/cuda.html">https://pytorch.org/docs/stable/notes/cuda.html</a></li>
                <li class="reference"> Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., ... & Zhang, Z. (2015). MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274.</li>
                <li class="reference"> Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... & Rush, A. M. (2020). Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 38-45).</li>
                <li class="reference"> PyTorch Hub Documentation. (2023). Torch Hub. <a href="https://pytorch.org/docs/stable/hub.html">https://pytorch.org/docs/stable/hub.html</a></li>
                <li class="reference"> He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).</li>
                <li class="reference"> Tan, C., Sun, F., Kong, T., Zhang, W., Yang, C., & Liu, C. (2018). A survey on deep transfer learning. In International conference on artificial neural networks (pp. 270-279). Springer, Cham.</li>
                <li class="reference"> PyTorch Documentation. (2023). TorchScript. <a href="https://pytorch.org/docs/stable/jit.html">https://pytorch.org/docs/stable/jit.html</a></li>
                <li class="reference"> Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., ... & He, Q. (2020). A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1), 43-76.</li>
                <li class="reference"> PyTorch Documentation. (2023). Quantization. <a href="https://pytorch.org/docs/stable/quantization.html">https://pytorch.org/docs/stable/quantization.html</a></li>
                <li class="reference"> Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., ... & Kalenichenko, D. (2018). Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2704-2713).</li>
                <li class="reference"> ONNX. (2023). Open Neural Network Exchange. <a href="https://onnx.ai">https://onnx.ai</a></li>
                <li class="reference"> ONNX Runtime. (2023). ONNX Runtime Documentation. <a href="https://onnxruntime.ai">https://onnxruntime.ai</a></li>
                <li class="reference"> Bai, J., Lu, F., Zhang, K., & Others. (2019, October). ONNX: Open Neural Network Exchange. In GitHub repository.</li>
                <li class="reference"> PyTorch Mobile. (2023). PyTorch Mobile Documentation. <a href="https://pytorch.org/mobile/home/">https://pytorch.org/mobile/home/</a></li>
                <li class="reference"> Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., ... & Adam, H. (2017). Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861.</li>
                <li class="reference"> PyTorch Documentation. (2023). Optimizing Performance. <a href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html">https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html</a></li>
                <li class="reference"> TorchServe. (2023). TorchServe Documentation. <a href="https://pytorch.org/serve/">https://pytorch.org/serve/</a></li>
                <li class="reference"> Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V., ... & Su, B. Y. (2014). Scaling distributed machine learning with the parameter server. In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14) (pp. 583-598).</li>
                <li class="reference"> Shankar, S., Acun, B., Foster, J. S., Kuditipudi, R., Renggli, C., Wu, X., ... & Povinelli, M. (2022, March). Serverless inference for machine learning serving. In 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (pp. 64-77).</li>
                <li class="reference"> Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D., ... & Dennison, D. (2015). Hidden technical debt in machine learning systems. Advances in neural information processing systems, 28.</li>
            </ol>
        </section>
    </body>
</html>