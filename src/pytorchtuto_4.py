# -*- coding: utf-8 -*-
"""PyTorchTuto_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cm33X_AeBdvo6V4Tja1zUZd99rzNQu0Z

# PyTorchTuto_4.ipynb

## Author: Akshay Anand, PhD Candidate  
**Florida State University (2021 - 20XX)**
[*Github*](https://github.com/anand-me)
---

## Introduction  

Welcome to the fourth notebook in the PyTorch tutorial series! In this session, we will build upon the concepts introduced in the previous notebooks and explore the training process in PyTorch.

This notebook will focus on:  
1. **Training Models**  
   - Loss Functions
   - Optimizers
   - Training Loops

By the end of this notebook, you will understand how to effectively train neural networks using PyTorch's optimization tools and best practices.  

---

Let‚Äôs get started!
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.metrics import confusion_matrix
import seaborn as sns
import time
from IPython.display import clear_output

# Commented out IPython magic to ensure Python compatibility.
# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Enable plots in the notebook
# %matplotlib inline

print(f"PyTorch version: {torch.__version__}")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ============================================================================ #
#                                1. LOSS FUNCTIONS                              #
# ============================================================================ #

print("\n" + "="*80)
print("1. LOSS FUNCTIONS".center(80))
print("="*80)

# ---------------------------------------------------------------------------- #
#                         Understanding Loss Functions                          #
# ---------------------------------------------------------------------------- #

print("\nüìå What are Loss Functions?")
print("Loss functions measure how far our model's predictions are from the true values.")
print("They provide the signal for updating model parameters during training.")

print("\nüìù Common Loss Functions in PyTorch:")

# Create some sample data
# For regression problems
y_true_reg = torch.tensor([0.5, 1.5, 2.5, 3.5, 4.5])
y_pred_reg1 = torch.tensor([0.4, 1.3, 2.3, 3.2, 4.3])  # Good predictions
y_pred_reg2 = torch.tensor([1.5, 2.5, 1.5, 5.0, 3.0])  # Poor predictions

# For binary classification
y_true_bin = torch.tensor([0, 1, 0, 1, 0], dtype=torch.float32)
y_pred_bin_good = torch.tensor([0.1, 0.9, 0.2, 0.8, 0.1], dtype=torch.float32)
y_pred_bin_bad = torch.tensor([0.7, 0.3, 0.6, 0.2, 0.8], dtype=torch.float32)

# For multi-class classification (one-hot encoded targets)
y_true_multi = torch.tensor([0, 2, 1, 1, 3])
# Convert to one-hot encoding for some loss functions
y_true_multi_onehot = F.one_hot(y_true_multi, num_classes=4).float()
# Predicted probabilities (good)
y_pred_multi_good = torch.tensor([
    [0.9, 0.05, 0.03, 0.02],  # Predicts class 0
    [0.05, 0.05, 0.85, 0.05],  # Predicts class 2
    [0.05, 0.85, 0.05, 0.05],  # Predicts class 1
    [0.05, 0.80, 0.05, 0.10],  # Predicts class 1
    [0.05, 0.05, 0.05, 0.85]   # Predicts class 3
])
# Predicted probabilities (bad)
y_pred_multi_bad = torch.tensor([
    [0.2, 0.3, 0.3, 0.2],     # Uncertain prediction
    [0.6, 0.3, 0.05, 0.05],   # Wrong prediction (class 0 instead of 2)
    [0.7, 0.1, 0.1, 0.1],     # Wrong prediction (class 0 instead of 1)
    [0.25, 0.25, 0.25, 0.25], # Very uncertain prediction
    [0.4, 0.4, 0.1, 0.1]      # Wrong prediction (not class 3)
])

# ---------------------------------------------------------------------------- #
#                          1.1 Regression Loss Functions                        #
# ---------------------------------------------------------------------------- #

print("\nüìä 1.1 Regression Loss Functions:")

# Mean Squared Error (MSE)
mse_loss = nn.MSELoss()
mse_good = mse_loss(y_pred_reg1, y_true_reg)
mse_bad = mse_loss(y_pred_reg2, y_true_reg)
print(f"  ‚Ä¢ Mean Squared Error (MSE)")
print(f"    - Good predictions MSE: {mse_good.item():.4f}")
print(f"    - Poor predictions MSE: {mse_bad.item():.4f}")

# Mean Absolute Error (L1 Loss)
mae_loss = nn.L1Loss()
mae_good = mae_loss(y_pred_reg1, y_true_reg)
mae_bad = mae_loss(y_pred_reg2, y_true_reg)
print(f"\n  ‚Ä¢ Mean Absolute Error (L1)")
print(f"    - Good predictions MAE: {mae_good.item():.4f}")
print(f"    - Poor predictions MAE: {mae_bad.item():.4f}")

# Smooth L1 Loss (Huber Loss)
smooth_l1_loss = nn.SmoothL1Loss()
smooth_l1_good = smooth_l1_loss(y_pred_reg1, y_true_reg)
smooth_l1_bad = smooth_l1_loss(y_pred_reg2, y_true_reg)
print(f"\n  ‚Ä¢ Smooth L1 Loss (Huber)")
print(f"    - Good predictions Smooth L1: {smooth_l1_good.item():.4f}")
print(f"    - Poor predictions Smooth L1: {smooth_l1_bad.item():.4f}")

# Visualizing regression losses
x = torch.linspace(-3, 3, 1000)
y_true = torch.zeros_like(x)

# Calculate different losses
mse_values = (x - y_true)**2
mae_values = torch.abs(x - y_true)
# Huber loss with delta=1
delta = 1.0
huber_values = torch.where(
    torch.abs(x - y_true) < delta,
    0.5 * (x - y_true)**2,
    delta * (torch.abs(x - y_true) - 0.5 * delta)
)

plt.figure(figsize=(12, 6))
plt.plot(x.numpy(), mse_values.numpy(), label='MSE Loss', linewidth=2)
plt.plot(x.numpy(), mae_values.numpy(), label='MAE Loss', linewidth=2)
plt.plot(x.numpy(), huber_values.numpy(), label='Huber Loss (delta=1)', linewidth=2)
plt.grid(True, alpha=0.3)
plt.legend()
plt.title('Comparison of Regression Loss Functions')
plt.xlabel('Prediction Error (y_pred - y_true)')
plt.ylabel('Loss Value')
plt.show()

# ---------------------------------------------------------------------------- #
#                    1.2 Binary Classification Loss Functions                   #
# ---------------------------------------------------------------------------- #

print("\nüìä 1.2 Binary Classification Loss Functions:")

# Binary Cross Entropy
bce_loss = nn.BCELoss()
bce_good = bce_loss(y_pred_bin_good, y_true_bin)
bce_bad = bce_loss(y_pred_bin_bad, y_true_bin)
print(f"  ‚Ä¢ Binary Cross Entropy (BCE)")
print(f"    - Good predictions BCE: {bce_good.item():.4f}")
print(f"    - Poor predictions BCE: {bce_bad.item():.4f}")

# BCE with Logits (combines sigmoid and BCE)
# Convert to logits (log-odds) for BCE with Logits
y_pred_bin_good_logits = torch.log(y_pred_bin_good / (1 - y_pred_bin_good))
y_pred_bin_bad_logits = torch.log(y_pred_bin_bad / (1 - y_pred_bin_bad))

bce_logits_loss = nn.BCEWithLogitsLoss()
bce_logits_good = bce_logits_loss(y_pred_bin_good_logits, y_true_bin)
bce_logits_bad = bce_logits_loss(y_pred_bin_bad_logits, y_true_bin)
print(f"\n  ‚Ä¢ BCE with Logits")
print(f"    - Good predictions BCE with Logits: {bce_logits_good.item():.4f}")
print(f"    - Poor predictions BCE with Logits: {bce_logits_bad.item():.4f}")

# Visualizing Binary Classification Loss
# Generate probability values from 0 to 1
p = torch.linspace(0.001, 0.999, 1000)

# Calculate BCE loss for y=1 and y=0
bce_y1 = -torch.log(p)
bce_y0 = -torch.log(1 - p)

plt.figure(figsize=(12, 6))
plt.plot(p.numpy(), bce_y1.numpy(), label='BCE Loss (y=1)', linewidth=2)
plt.plot(p.numpy(), bce_y0.numpy(), label='BCE Loss (y=0)', linewidth=2)
plt.grid(True, alpha=0.3)
plt.legend()
plt.title('Binary Cross Entropy Loss')
plt.xlabel('Predicted Probability')
plt.ylabel('Loss Value')
plt.xlim(0, 1)
plt.ylim(0, 5)
plt.show()

# ---------------------------------------------------------------------------- #
#                  1.3 Multi-class Classification Loss Functions                #
# ---------------------------------------------------------------------------- #

print("\nüìä 1.3 Multi-class Classification Loss Functions:")

# Cross Entropy Loss
ce_loss = nn.CrossEntropyLoss()
# For CrossEntropyLoss, we need class indices as targets (not one-hot)
# and logits (raw scores) as predictions
ce_good = ce_loss(y_pred_multi_good, y_true_multi)
ce_bad = ce_loss(y_pred_multi_bad, y_true_multi)
print(f"  ‚Ä¢ Cross Entropy")
print(f"    - Good predictions Cross Entropy: {ce_good.item():.4f}")
print(f"    - Poor predictions Cross Entropy: {ce_bad.item():.4f}")

# Kullback-Leibler Divergence Loss
kl_loss = nn.KLDivLoss(reduction='batchmean')
# KLDivLoss expects log probabilities as input
y_pred_multi_good_log = torch.log(y_pred_multi_good)
y_pred_multi_bad_log = torch.log(y_pred_multi_bad)
kl_good = kl_loss(y_pred_multi_good_log, y_true_multi_onehot)
kl_bad = kl_loss(y_pred_multi_bad_log, y_true_multi_onehot)
print(f"\n  ‚Ä¢ KL Divergence")
print(f"    - Good predictions KL Divergence: {kl_good.item():.4f}")
print(f"    - Poor predictions KL Divergence: {kl_bad.item():.4f}")

# ---------------------------------------------------------------------------- #
#                         1.4 Custom Loss Functions                             #
# ---------------------------------------------------------------------------- #

print("\nüìä 1.4 Custom Loss Functions:")
print("  ‚Ä¢ You can create custom loss functions by extending nn.Module")
print("  ‚Ä¢ Or by defining a function that operates on tensors")

# Example: Focal Loss (custom implementation)
# Helps with class imbalance by reducing the loss contribution from easy examples
def focal_loss(predictions, targets, gamma=2.0, alpha=0.25):
    """
    Focal Loss for binary classification.

    Args:
        predictions: Tensor of predicted probabilities
        targets: Tensor of ground truth labels (0 or 1)
        gamma: Focusing parameter (higher means more focus on hard examples)
        alpha: Weighting factor for the positive class

    Returns:
        Computed focal loss
    """
    # Ensure predictions are probabilities
    predictions = torch.clamp(predictions, min=1e-7, max=1-1e-7)

    # Binary cross entropy component
    bce = -targets * torch.log(predictions) - (1 - targets) * torch.log(1 - predictions)

    # Focal component
    pt = torch.where(targets == 1, predictions, 1 - predictions)
    focal_weight = (1 - pt) ** gamma

    # Apply alpha weighting
    alpha_weight = torch.where(targets == 1, alpha, 1 - alpha)

    # Combine all components
    loss = alpha_weight * focal_weight * bce

    return loss.mean()

# Let's compare standard BCE vs Focal Loss
focal_good = focal_loss(y_pred_bin_good, y_true_bin)
focal_bad = focal_loss(y_pred_bin_bad, y_true_bin)
print(f"\n  ‚Ä¢ Focal Loss Comparison:")
print(f"    - Standard BCE (good predictions): {bce_good.item():.4f}")
print(f"    - Focal Loss (good predictions): {focal_good.item():.4f}")
print(f"    - Standard BCE (poor predictions): {bce_bad.item():.4f}")
print(f"    - Focal Loss (poor predictions): {focal_bad.item():.4f}")
print(f"    - Notice how Focal Loss emphasizes hard examples")

# Create a class-based custom loss
class WeightedMSELoss(nn.Module):
    def __init__(self, weight=None):
        super(WeightedMSELoss, self).__init__()
        self.weight = weight

    def forward(self, predictions, targets):
        if self.weight is None:
            return torch.mean((predictions - targets) ** 2)
        else:
            return torch.mean(self.weight * (predictions - targets) ** 2)

# Example with weighted MSE loss
weights = torch.tensor([0.5, 1.0, 1.5, 2.0, 2.5])  # Higher weights for later samples
weighted_mse = WeightedMSELoss(weight=weights)
weighted_mse_good = weighted_mse(y_pred_reg1, y_true_reg)
weighted_mse_bad = weighted_mse(y_pred_reg2, y_true_reg)

print(f"\n  ‚Ä¢ Weighted MSE (Custom Loss):")
print(f"    - Standard MSE (good predictions): {mse_good.item():.4f}")
print(f"    - Weighted MSE (good predictions): {weighted_mse_good.item():.4f}")
print(f"    - Standard MSE (poor predictions): {mse_bad.item():.4f}")
print(f"    - Weighted MSE (poor predictions): {weighted_mse_bad.item():.4f}")

# ============================================================================ #
#                                  2. OPTIMIZERS                                #
# ============================================================================ #

print("\n" + "="*80)
print("2. OPTIMIZERS".center(80))
print("="*80)

# ---------------------------------------------------------------------------- #
#                         Understanding Optimizers                              #
# ---------------------------------------------------------------------------- #

print("\nüìå What are Optimizers?")
print("Optimizers adjust model parameters based on the computed gradients to minimize the loss function.")
print("They implement different algorithms for updating weights during training.")

# ---------------------------------------------------------------------------- #
#                           2.1 Common Optimizers                               #
# ---------------------------------------------------------------------------- #

print("\nüìä 2.1 Common Optimizers in PyTorch:")

# Create a simple model to demonstrate optimizers
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(2, 5)
        self.fc2 = nn.Linear(5, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Create multiple instances of the same model
model_sgd = SimpleModel()
model_momentum = SimpleModel()
model_adagrad = SimpleModel()
model_rmsprop = SimpleModel()
model_adam = SimpleModel()

# Copy parameters to ensure all models start from the same point
with torch.no_grad():
    for param_sgd, param_momentum, param_adagrad, param_rmsprop, param_adam in zip(
        model_sgd.parameters(),
        model_momentum.parameters(),
        model_adagrad.parameters(),
        model_rmsprop.parameters(),
        model_adam.parameters()):

        # Initialize with the same parameters
        init_param = torch.randn_like(param_sgd)
        param_sgd.copy_(init_param)
        param_momentum.copy_(init_param)
        param_adagrad.copy_(init_param)
        param_rmsprop.copy_(init_param)
        param_adam.copy_(init_param)

# Create different optimizers
sgd = optim.SGD(model_sgd.parameters(), lr=0.01)
momentum = optim.SGD(model_momentum.parameters(), lr=0.01, momentum=0.9)
adagrad = optim.Adagrad(model_adagrad.parameters(), lr=0.01)
rmsprop = optim.RMSprop(model_rmsprop.parameters(), lr=0.01)
adam = optim.Adam(model_adam.parameters(), lr=0.01)

print("\n  ‚Ä¢ Stochastic Gradient Descent (SGD)")
print(f"    {sgd}")
print("\n  ‚Ä¢ SGD with Momentum")
print(f"    {momentum}")
print("\n  ‚Ä¢ Adagrad")
print(f"    {adagrad}")
print("\n  ‚Ä¢ RMSprop")
print(f"    {rmsprop}")
print("\n  ‚Ä¢ Adam")
print(f"    {adam}")

# ---------------------------------------------------------------------------- #
#                       2.2 Comparing Optimizers                                #
# ---------------------------------------------------------------------------- #

print("\nüìä 2.2 Comparing Optimizer Performance:")

# Create synthetic data for a simple regression problem
np.random.seed(42)
X = np.random.rand(1000, 2) * 10
y = 3 * X[:, 0] + 2 * X[:, 1] + 1 + np.random.randn(1000) * 2

# Convert to PyTorch tensors
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)

# Loss function
criterion = nn.MSELoss()

# Training loop for comparing optimizers
epochs = 100
optimizers = {
    'SGD': (model_sgd, sgd),
    'Momentum': (model_momentum, momentum),
    'Adagrad': (model_adagrad, adagrad),
    'RMSprop': (model_rmsprop, rmsprop),
    'Adam': (model_adam, adam)
}

history = {name: [] for name in optimizers.keys()}

for epoch in range(epochs):
    for name, (model, optimizer) in optimizers.items():
        # Forward pass
        y_pred = model(X_tensor)
        loss = criterion(y_pred, y_tensor)

        # Record loss
        history[name].append(loss.item())

        # Backward pass and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Print progress every 10 epochs
    if (epoch+1) % 20 == 0:
        print(f"Epoch {epoch+1}/{epochs}")
        for name in optimizers.keys():
            print(f"  ‚Ä¢ {name}: Loss = {history[name][-1]:.4f}")

# Plot the learning curves
plt.figure(figsize=(12, 6))
for name, losses in history.items():
    plt.plot(losses, label=name)
plt.title('Optimizer Comparison')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# ---------------------------------------------------------------------------- #
#                       2.3 Learning Rate Scheduling                            #
# ---------------------------------------------------------------------------- #

print("\nüìä 2.3 Learning Rate Scheduling:")
print("  ‚Ä¢ Learning rate schedulers adjust the learning rate during training")
print("  ‚Ä¢ Common strategies: time-based decay, step decay, exponential decay")

# Create a new model for demonstration
model_scheduler = SimpleModel()

# Create optimizer
optimizer = optim.SGD(model_scheduler.parameters(), lr=0.1)

# Create different types of schedulers
step_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
exp_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)
plateau_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)
cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

# Let's visualize the learning rate schedules
epochs = 100
lr_history = {
    'Step LR': [],
    'Exponential LR': [],
    'Cosine Annealing': []
}

# Reset optimizer's learning rate
for param_group in optimizer.param_groups:
    param_group['lr'] = 0.1

# Step LR Schedule
step_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
for epoch in range(epochs):
    lr_history['Step LR'].append(optimizer.param_groups[0]['lr'])
    step_scheduler.step()

# Reset optimizer's learning rate
for param_group in optimizer.param_groups:
    param_group['lr'] = 0.1

# Exponential LR Schedule
exp_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)
for epoch in range(epochs):
    lr_history['Exponential LR'].append(optimizer.param_groups[0]['lr'])
    exp_scheduler.step()

# Reset optimizer's learning rate
for param_group in optimizer.param_groups:
    param_group['lr'] = 0.1

# Cosine Annealing LR Schedule
cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
for epoch in range(epochs):
    lr_history['Cosine Annealing'].append(optimizer.param_groups[0]['lr'])
    cosine_scheduler.step()

# Plot the learning rate schedules
plt.figure(figsize=(12, 6))
for name, lr_values in lr_history.items():
    plt.plot(lr_values, label=name)
plt.title('Learning Rate Scheduling Strategies')
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# ============================================================================ #
#                                 3. TRAINING LOOPS                             #
# ============================================================================ #

print("\n" + "="*80)
print("3. TRAINING LOOPS".center(80))
print("="*80)

# ---------------------------------------------------------------------------- #
#                       3.1 Basic Training Loop                                 #
# ---------------------------------------------------------------------------- #

print("\nüìå 3.1 Basic Training Loop:")
print("  ‚Ä¢ The training loop is where everything comes together")
print("  ‚Ä¢ Core components: forward pass, loss computation, backward pass, parameter updates")

# Load MNIST dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# Load smaller subsets of data for quick demonstration
train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

# Use a smaller subset for faster execution
train_subset, _ = random_split(train_dataset, [10000, 50000])
test_subset, _ = random_split(test_dataset, [1000, 9000])

train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_subset, batch_size=1000)

# Define a simple CNN model
class MnistCNN(nn.Module):
    def __init__(self):
        super(MnistCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.dropout = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.dropout(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

# Instantiate the model
model = MnistCNN().to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

print("\nüìù Basic Training Loop Example (MNIST):")

# Define training and testing functions
def train(model, train_loader, optimizer, criterion, epoch):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        output = model(data)

        # Compute loss
        loss = criterion(output, target)

        # Backward pass
        loss.backward()

        # Update parameters
        optimizer.step()

        # Track loss and accuracy
        running_loss += loss.item()
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()

        # Print progress
        if (batch_idx + 1) % 50 == 0:
            print(f'Epoch: {epoch} | Batch: {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f} | Acc: {100.*correct/total:.2f}%')

    return running_loss / len(train_loader), 100. * correct / total

def test(model, test_loader, criterion):
    model.eval()
    test_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)

            # Forward pass
            output = model(data)

            # Compute loss
            test_loss += criterion(output, target).item()

            # Compute accuracy
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

    # Average loss
    test_loss /= len(test_loader)
    test_accuracy = 100. * correct / total

    print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_accuracy:.2f}%')
    return test_loss, test_accuracy

# Train for a few epochs
num_epochs = 3
train_losses = []
train_accs = []
test_losses = []
test_accs = []

print("\nStarting training loop...\n")

for epoch in range(1, num_epochs + 1):
    # Train the model
    train_loss, train_acc = train(model, train_loader, optimizer, criterion, epoch)
    train_losses.append(train_loss)
    train_accs.append(train_acc)

    # Test the model
    test_loss, test_acc = test(model, test_loader, criterion)
    test_losses.append(test_loss)
    test_accs.append(test_acc)

    print(f'Epoch {epoch} summary:')
    print(f'Training Loss: {train_loss:.4f} | Training Acc: {train_acc:.2f}%')
    print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')
    print('-' * 60)

# Plot training progress
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(test_losses, label='Test Loss')
plt.title('Loss vs. Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(train_accs, label='Train Accuracy')
plt.plot(test_accs, label='Test Accuracy')
plt.title('Accuracy vs. Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()