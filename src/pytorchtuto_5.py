# -*- coding: utf-8 -*-
"""PyTorchTuto_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17y_7yrM2OyeuioussVgcZBaMcaTvBOLG

# PyTorchTuto_5.ipynb

## Author: Akshay Anand, PhD Candidate  
**Florida State University (2021 - 20XX)**
[*Github*](https://github.com/anand-me)
---

## Introduction  

Welcome to the fifth notebook in the PyTorch tutorial series! In this final session, we will learn how to save and load trained models, an essential skill for deploying your machine learning solutions.

This notebook will focus on:  
1. **Saving and Loading Models**  
   - Model Serialization in PyTorch
   - Loading Pretrained Models
   - Model Deployment Basics

By the end of this notebook, you will understand how to preserve your trained models for future use and deployment in real-world applications..  

---

Letâ€™s get started!
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
import os
import time
import io
import requests
from PIL import Image
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, random_split
from torchvision.utils import make_grid
import torchvision.transforms.functional as TF
import copy

# Commented out IPython magic to ensure Python compatibility.
# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Enable plots in the notebook
# %matplotlib inline

print(f"PyTorch version: {torch.__version__}")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ============================================================================ #
#                         1. MODEL SERIALIZATION IN PYTORCH                     #
# ============================================================================ #

print("\n" + "="*80)
print("1. MODEL SERIALIZATION IN PYTORCH".center(80))
print("="*80)

# ---------------------------------------------------------------------------- #
#                      1.1 Saving and Loading Model Weights                     #
# ---------------------------------------------------------------------------- #

print("\nðŸ“Œ 1.1 Saving and Loading Model Weights:")

# First, let's create a simple model to work with
class SimpleModel(nn.Module):
    def __init__(self, input_size=784, hidden_size=128, output_size=10):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = x.view(x.size(0), -1)  # Flatten the input
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

# Create model instance
model = SimpleModel().to(device)
print(model)

# Let's see the model's parameters
print("\nModel Parameters:")
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")

# 1. Saving only the model weights
print("\nðŸ“Š Method 1: Saving only the model weights")
print("  â€¢ Use torch.save() with model.state_dict()")
print("  â€¢ Advantages: Compact file size, flexible for loading")
print("  â€¢ Best for: Production environments, model sharing")

# Save model weights
weights_path = 'model_weights.pth'
torch.save(model.state_dict(), weights_path)
print(f"Model weights saved to {weights_path}")
print(f"File size: {os.path.getsize(weights_path) / 1024:.2f} KB")

# Load model weights into a new model instance
new_model = SimpleModel().to(device)
new_model.load_state_dict(torch.load(weights_path))
new_model.eval()  # Set to evaluation mode
print("\nNew model loaded with saved weights.")

# Verify the weights are the same
print("\nVerifying weights are the same:")
for (name1, param1), (name2, param2) in zip(model.named_parameters(), new_model.named_parameters()):
    assert torch.allclose(param1, param2), f"Parameters {name1} don't match!"
    print(f"{name1}: Weights match âœ“")

# ---------------------------------------------------------------------------- #
#                    1.2 Saving and Loading the Entire Model                    #
# ---------------------------------------------------------------------------- #

print("\nðŸ“Œ 1.2 Saving and Loading the Entire Model:")
print("  â€¢ Use torch.save() with the entire model")
print("  â€¢ Advantages: Includes model architecture")
print("  â€¢ Best for: Quick prototyping, personal use")

# Save the entire model
model_path = 'complete_model.pth'
torch.save(model, model_path)
print(f"Complete model saved to {model_path}")
print(f"File size: {os.path.getsize(model_path) / 1024:.2f} KB")

# Method 2: Explicitly set weights_only=False (less secure but works for older code)
try:
        loaded_model = torch.load(model_path, weights_only=False)
        loaded_model.eval()  # Set to evaluation mode
        print("Complete model loaded with weights_only=False.")
        print("Note: This method executes arbitrary code and should only be used with trusted model files.")
except Exception as e2:
        print(f"Error with weights_only=False approach: {e2}")

print("\nLoaded model architecture:")
print(loaded_model)

# ---------------------------------------------------------------------------- #
#               1.3 Saving Model with Training Information                      #
# ---------------------------------------------------------------------------- #

print("\nðŸ“Œ 1.3 Saving Model with Training Information:")
print("  â€¢ Save model weights, optimizer state, epochs, and other info")
print("  â€¢ Advantages: Allows resuming training from where you left off")
print("  â€¢ Best for: Long training processes that might be interrupted")

# Let's simulate some training first
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# Simulate a forward and backward pass
dummy_input = torch.randn(64, 784).to(device)
dummy_target = torch.randint(0, 10, (64,)).to(device)

# Forward pass
output = model(dummy_input)
loss = criterion(output, dummy_target)

# Backward pass
optimizer.zero_grad()
loss.backward()
optimizer.step()

# Now save model with training info
checkpoint = {
    'epoch': 10,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss.item(),
    'learning_rate': optimizer.param_groups[0]['lr'],
    'batch_size': 64
}

checkpoint_path = 'training_checkpoint.pth'
torch.save(checkpoint, checkpoint_path)
print(f"Training checkpoint saved to {checkpoint_path}")
print(f"File size: {os.path.getsize(checkpoint_path) / 1024:.2f} KB")

# Load the checkpoint
checkpoint = torch.load(checkpoint_path)
print("\nCheckpoint loaded with the following information:")
for key, value in checkpoint.items():
    if key != 'model_state_dict' and key != 'optimizer_state_dict':
        print(f"  â€¢ {key}: {value}")

# Load the model and optimizer states
resumed_model = SimpleModel().to(device)
resumed_model.load_state_dict(checkpoint['model_state_dict'])

resumed_optimizer = optim.Adam(resumed_model.parameters())
resumed_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

print("\nModel and optimizer states restored successfully.")
print(f"Current learning rate: {resumed_optimizer.param_groups[0]['lr']}")

# ---------------------------------------------------------------------------- #
#                     1.4 Saving Models for Different Devices                   #
# ---------------------------------------------------------------------------- #

print("\nðŸ“Œ 1.4 Saving Models for Different Devices:")
print("  â€¢ Models trained on GPU might not load correctly on CPU")
print("  â€¢ Use map_location to control where tensors are placed")

# Save the model that's on GPU/CPU
device_specific_path = 'device_model.pth'
torch.save(model.state_dict(), device_specific_path)

# Load to CPU regardless of where it was saved
cpu_model = SimpleModel()
cpu_model.load_state_dict(torch.load(device_specific_path, map_location=torch.device('cpu')))
print("Model saved from device and loaded to CPU successfully.")

# Load to GPU if available
gpu_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
gpu_model = SimpleModel()
gpu_model.load_state_dict(torch.load(device_specific_path, map_location=gpu_device))
gpu_model = gpu_model.to(gpu_device)
print(f"Model loaded to {gpu_device} successfully.")

# ============================================================================ #
#                          2. LOADING PRETRAINED MODELS                         #
# ============================================================================ #

print("\n" + "="*80)
print("2. LOADING PRETRAINED MODELS".center(80))
print("="*80)

# ---------------------------------------------------------------------------- #
#                    2.1 Using Pretrained Models from torchvision               #
# ---------------------------------------------------------------------------- #

print("\nðŸ“Œ 2.1 Using Pretrained Models from torchvision:")
print("  â€¢ PyTorch provides many pretrained models through torchvision")
print("  â€¢ These models are trained on ImageNet and other datasets")

# List available pretrained models
print("\nSome available pretrained models in torchvision:")
print("  â€¢ ResNet: models.resnet18(), models.resnet50(), etc.")
print("  â€¢ VGG: models.vgg16(), models.vgg19(), etc.")
print("  â€¢ DenseNet: models.densenet121(), etc.")
print("  â€¢ MobileNet: models.mobilenet_v2(), etc.")
print("  â€¢ EfficientNet: models.efficientnet_b0(), etc.")

# Load a pretrained ResNet model
print("\nLoading a pretrained ResNet18 model:")
resnet = models.resnet18(pretrained=True)
resnet.eval()  # Set to evaluation mode

# Check the model architecture
print(f"\nResNet18 loaded with {sum(p.numel() for p in resnet.parameters())} parameters")
print(f"Last layer: {resnet.fc}")

# ---------------------------------------------------------------------------- #
#                        2.2 Modifying Pretrained Models                        #
# ---------------------------------------------------------------------------- #

print("\nðŸ“Œ 2.2 Modifying Pretrained Models:")
print("  â€¢ Adapt pretrained models for your specific tasks")
print("  â€¢ Common techniques: feature extraction and fine-tuning")

# Feature Extraction - freeze all layers except the final layer
print("\nðŸ“Š Method 1: Feature Extraction")
print("  â€¢ Freeze all layers except the final classification layer")
print("  â€¢ Use the model as a fixed feature extractor")

feature_extractor = models.resnet18(pretrained=True)

# Freeze all layers
for param in feature_extractor.parameters():
    param.requires_grad = False

# Modify the final layer for a new task (e.g., 5 classes instead of 1000)
num_features = feature_extractor.fc.in_features
feature_extractor.fc = nn.Linear(num_features, 5)

# Verify that only fc parameters are trainable
trainable_params = [name for name, param in feature_extractor.named_parameters() if param.requires_grad]
print(f"\nTrainable layers in feature extractor: {trainable_params}")
print(f"Only {sum(p.numel() for p in feature_extractor.parameters() if p.requires_grad)} parameters are trainable")
print(f"Out of {sum(p.numel() for p in feature_extractor.parameters())} total parameters")

# Fine-tuning - train all layers with different learning rates
print("\nðŸ“Š Method 2: Fine-tuning")
print("  â€¢ Train all layers but with different learning rates")
print("  â€¢ Earlier layers learn slower than later layers")

fine_tuned_model = models.resnet18(pretrained=True)

fine_tuned_model = models.resnet18(pretrained=True)

# Group parameters by layers
fc_params = list(fine_tuned_model.fc.parameters())
backbone_params = list(filter(lambda p: id(p) not in [id(fc_param) for fc_param in fc_params],
                              fine_tuned_model.parameters()))

# Create optimizer with different learning rates
optimizer = optim.SGD([
    {'params': backbone_params, 'lr': 1e-4},  # Slower learning for pretrained layers
    {'params': fc_params, 'lr': 1e-3}         # Faster learning for new layers
], momentum=0.9)

print(f"\nLearning rates in optimizer:")
for i, param_group in enumerate(optimizer.param_groups):
    print(f"  â€¢ Group {i+1}: lr={param_group['lr']}")

# ---------------------------------------------------------------------------- #
#                    2.3 Using Custom Pretrained Models                         #
# ---------------------------------------------------------------------------- #

print("\nðŸ“Œ 2.3 Using Custom Pretrained Models:")
print("  â€¢ Load models from your own or others' training")
print("  â€¢ Useful for transfer learning in similar domains")

# Save our modified model as a "pretrained" model
custom_pretrained_path = 'custom_pretrained.pth'
torch.save(fine_tuned_model.state_dict(), custom_pretrained_path)
print(f"Custom pretrained model saved to {custom_pretrained_path}")

# Load the custom pretrained model
custom_model = models.resnet18(pretrained=False)  # Start with non-pretrained model

# First load the state dictionary
state_dict = torch.load(custom_pretrained_path)

# Check if there's a size mismatch in the final layer
if 'fc.weight' in state_dict and state_dict['fc.weight'].size(0) != 5:
    print("The saved model has a different number of output classes than our model.")
    print(f"Saved model has {state_dict['fc.weight'].size(0)} classes, but we want 5 classes.")

    # Extract all parameters except the final fully connected layer
    pretrained_dict = {k: v for k, v in state_dict.items() if 'fc.' not in k}

    # Modify the model's FC layer to match what we want
    custom_model.fc = nn.Linear(custom_model.fc.in_features, 5)

    # Get the current model state dict and update with pretrained values
    model_dict = custom_model.state_dict()
    model_dict.update(pretrained_dict)

    # Load the updated state dict
    custom_model.load_state_dict(model_dict, strict=False)
    print("Loaded pretrained features while keeping our custom classification layer.")
else:
    # If there's no mismatch, load normally
    custom_model.fc = nn.Linear(custom_model.fc.in_features, 5)
    custom_model.load_state_dict(state_dict)
    print("Custom pretrained model loaded successfully with matching architecture.")

custom_model.eval()

print("Custom pretrained model loaded successfully.")

# ============================================================================ #
#                           3. MODEL DEPLOYMENT BASICS                          #
# ============================================================================ #

print("\n" + "="*80)
print("3. MODEL DEPLOYMENT BASICS".center(80))
print("="*80)

# ---------------------------------------------------------------------------- #
#                        3.1 Model Export with TorchScript                      #
# ---------------------------------------------------------------------------- #

print("\nðŸ“Œ 3.1 Model Export with TorchScript:")
print("  â€¢ TorchScript: A way to serialize and optimize PyTorch models")
print("  â€¢ Enables deployment in production environments without Python")
print("  â€¢ Two methods: tracing and scripting")

# Create a simple model to demonstrate TorchScript
class ScriptableModel(nn.Module):
    def __init__(self):
        super(ScriptableModel, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        return x

# Create model instance
scriptable_model = ScriptableModel().to(device)
scriptable_model.eval()

# Method 1: Tracing
print("\nðŸ“Š Method 1: Tracing")
print("  â€¢ Runs the model once and records operations")
print("  â€¢ Best for models with static control flow")

# Create an example input for tracing
example_input = torch.rand(1, 1, 28, 28).to(device)

# Trace the model
traced_script_module = torch.jit.trace(scriptable_model, example_input)
print("Model traced successfully.")

# Save the traced model
traced_model_path = 'traced_model.pt'
traced_script_module.save(traced_model_path)
print(f"Traced model saved to {traced_model_path}")

# Method 2: Scripting
print("\nðŸ“Š Method 2: Scripting")
print("  â€¢ Directly analyzes the code")
print("  â€¢ Best for models with dynamic control flow")

# Script the model
scripted_model = torch.jit.script(scriptable_model)
print("Model scripted successfully.")

# Save the scripted model
scripted_model_path = 'scripted_model.pt'
scripted_model.save(scripted_model_path)
print(f"Scripted model saved to {scripted_model_path}")

# Load the TorchScript model
loaded_script_model = torch.jit.load(traced_model_path)
print("\nTorchScript model loaded successfully.")

# Verify it still works
test_input = torch.rand(1, 1, 28, 28).to(device)
with torch.no_grad():
    original_output = scriptable_model(test_input)
    loaded_output = loaded_script_model(test_input)

assert torch.allclose(original_output, loaded_output)
print("Verification passed: Original and loaded model outputs match.")

# ---------------------------------------------------------------------------- #
#                      3.2 Model Quantization for Efficiency                    #
# ---------------------------------------------------------------------------- #

print("\nðŸ“Œ 3.2 Model Quantization for Efficiency:")
print("  â€¢ Reduce model size and increase inference speed")
print("  â€¢ Convert floating-point to integer arithmetic")
print("  â€¢ PyTorch supports various quantization methods")

# Let's use a simple model for quantization demonstration
model_to_quantize = models.resnet18(pretrained=True).to('cpu')
model_to_quantize.eval()

# Post-training static quantization
print("\nðŸ“Š Post-training static quantization:")
print("  â€¢ This reduces model size and increases inference speed")

# Define a calibration function to run through representative data
def calibrate(model):
    # Generate random calibration data
    # Note: In practice, use real data representative of inference
    calibration_data = torch.randn(20, 3, 224, 224)
    with torch.no_grad():
        for data in calibration_data:
            model(data.unsqueeze(0))

# Prepare the model for static quantization
quantized_model = torch.quantization.quantize_dynamic(
    model_to_quantize,
    {nn.Linear, nn.Conv2d},  # Specify which layers to quantize
    dtype=torch.qint8
)

print(f"\nOriginal model size: {sum(p.numel() for p in model_to_quantize.parameters()) * 4 / (1024 * 1024):.2f} MB (float32)")
print(f"Quantized model size: {sum(p.numel() for p in quantized_model.parameters()) / (1024 * 1024):.2f} MB (int8 equivalent)")

# ---------------------------------------------------------------------------- #
#                     3.3 Optimizing Model for Inference                        #
# ---------------------------------------------------------------------------- #

print("\nðŸ“Œ 3.3 Optimizing Model for Inference:")
print("  â€¢ Switch model to evaluation mode")
print("  â€¢ Use torch.no_grad() for inference")
print("  â€¢ Batch inputs for efficient processing")

def benchmark_inference(model, input_size, batch_size, num_iterations=100):
    model.eval()  # Set to evaluation mode
    # Make sure the input tensor is on the same device as the model
    device = next(model.parameters()).device
    input_tensor = torch.randn(batch_size, *input_size).to(device)

    # Warmup
    with torch.no_grad():
        for _ in range(10):
            _ = model(input_tensor)  # Use underscore to discard the output

    # Benchmark
    start_time = time.time()
    with torch.no_grad():
        for _ in range(num_iterations):
            _ = model(input_tensor)
    end_time = time.time()

    return (end_time - start_time) / num_iterations

# Create a simple model for benchmarking
benchmark_model = nn.Sequential(
    nn.Conv2d(3, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2),
    nn.Conv2d(64, 128, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2),
    nn.Flatten(),
    nn.Linear(128 * 56 * 56, 1000)
).to(device)

# Benchmark with different batch sizes
batch_sizes = [1, 4, 16, 32]
input_size = (3, 224, 224)

print("\nðŸ“Š Inference Speed with Different Batch Sizes:")
for batch_size in batch_sizes:
    inference_time = benchmark_inference(benchmark_model, input_size, batch_size)
    print(f"  â€¢ Batch size {batch_size}: {inference_time * 1000:.2f} ms per batch "
          f"({inference_time * 1000 / batch_size:.2f} ms per image)")

# ---------------------------------------------------------------------------- #
#                        3.4 Model Serving Example                              #
# ---------------------------------------------------------------------------- #

print("\nðŸ“Œ 3.4 Model Serving Example:")
print("  â€¢ In a real deployment, you'd serve the model via API")
print("  â€¢ Common frameworks: TorchServe, Flask, FastAPI, ONNX Runtime")

# We'll create a simple example of model serving with a function
def model_prediction_service(model, image_path=None, image_url=None, image_tensor=None):
    """Simulates a model serving API for image classification.

    Args:
        model: The PyTorch model to use
        image_path: Path to a local image file
        image_url: URL to an image
        image_tensor: Direct tensor input

    Returns:
        Dictionary with class predictions
    """
    # Set model to evaluation mode
    model.eval()

    # Image transforms (same as used for training)
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    # Load image based on input type
    if image_tensor is not None:
        input_tensor = image_tensor
    elif image_path is not None:
        input_image = Image.open(image_path).convert('RGB')
        input_tensor = transform(input_image).unsqueeze(0)
    elif image_url is not None:
        response = requests.get(image_url)
        input_image = Image.open(io.BytesIO(response.content)).convert('RGB')
        input_tensor = transform(input_image).unsqueeze(0)
    else:
        raise ValueError("Must provide one of: image_path, image_url, or image_tensor")

    # Move tensor to correct device
    input_tensor = input_tensor.to(next(model.parameters()).device)

    # Perform inference
    with torch.no_grad():
        output = model(input_tensor)
        probabilities = torch.nn.functional.softmax(output, dim=1)
        top5_prob, top5_idx = torch.topk(probabilities, 5)

    # Return results
    return {
        "top_predictions": [
            {"class_id": idx.item(), "probability": prob.item()}
            for idx, prob in zip(top5_idx[0], top5_prob[0])
        ]
    }

# For demonstration purposes, we'll create a synthetic image
synthetic_image = torch.randn(1, 3, 224, 224)

# Use a pretrained model for prediction
print("\nðŸ“Š Example of Model Serving:")
print("  â€¢ Using a ResNet18 model to classify an image")

# Load a small pretrained model
inference_model = models.resnet18(pretrained=True).to(device)
inference_model.eval()

# Make a prediction
prediction = model_prediction_service(
    model=inference_model,
    image_tensor=synthetic_image
)

print("\nPrediction results:")
for i, pred in enumerate(prediction["top_predictions"]):
    print(f"  â€¢ Rank {i+1}: Class {pred['class_id']}, Probability: {pred['probability']:.4f}")

# ---------------------------------------------------------------------------- #
#                             3.5 Deployment Tips                               #
# ---------------------------------------------------------------------------- #

print("\nðŸ“Œ 3.5 Deployment Tips:")
print("\nðŸ“Š Export options:")
print("  â€¢ TorchScript (.pt): For C++ deployment")
print("  â€¢ ONNX (.onnx): For cross-framework compatibility")
print("  â€¢ TensorRT: For NVIDIA GPUs optimization")
print("  â€¢ CoreML/TFLite: For mobile deployment")

print("\nðŸ“Š Deployment considerations:")
print("  â€¢ Hardware requirements (CPU, GPU, memory)")
print("  â€¢ Latency vs throughput trade-offs")
print("  â€¢ Scaling (horizontal vs vertical)")
print("  â€¢ Model monitoring and updating")

print("\nðŸ“Š Best practices:")
print("  â€¢ Version your models")
print("  â€¢ Monitor performance metrics")
print("  â€¢ Implement A/B testing for updates")
print("  â€¢ Set up CI/CD pipelines for model deployment")

# ============================================================================ #
#                                BONUS: MODEL HUB                               #
# ============================================================================ #

print("\n" + "="*80)
print("BONUS: MODEL HUB".center(80))
print("="*80)

print("\nðŸ“Œ PyTorch Hub:")
print("  â€¢ Central repository for pretrained models")
print("  â€¢ Easy way to share and use models from the community")

print("\nExamples of models available on PyTorch Hub:")
print("  â€¢ Vision: ResNet, DenseNet, DeepLabV3, YOLO")
print("  â€¢ NLP: BERT, GPT-2, T5, RoBERTa")
print("  â€¢ Audio: Wav2Vec, Tacotron2")
print("  â€¢ And many more!")

print("\nUsing a model from PyTorch Hub:")
print("  model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)")
print("  model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')")

# Commented out IPython magic to ensure Python compatibility.
# ============================================================================ #
#                    BONUS: USING TENSORBOARD IN JUPYTER/COLAB                  #
# ============================================================================ #

print("\n" + "="*80)
print("BONUS: USING TENSORBOARD IN JUPYTER/COLAB".center(80))
print("="*80)

# Install TensorBoard if not already installed
!pip install -q tensorboard

# For Google Colab, load the extension
try:
#     %load_ext tensorboard
    print("TensorBoard extension loaded successfully!")
except:
    print("Warning: Could not load TensorBoard extension. If using regular Jupyter, this is normal.")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms, utils  # Added utils here
import numpy as np
import datetime

print("\nðŸ“Œ Setting up TensorBoard:")
print("  â€¢ SummaryWriter: Main interface to log data for TensorBoard")
print("  â€¢ You can log: scalars, images, histograms, graphs, embeddings, etc.")

# Create a unique directory name with timestamp
log_dir = "runs/tensorboard_demo_" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
writer = SummaryWriter(log_dir)
print(f"TensorBoard logs will be saved to: {log_dir}")

# Get a sample batch for visualization
dataiter = iter(train_loader)
images, labels = next(dataiter)

# Log a batch of images to TensorBoard
grid = utils.make_grid(images)  # Fixed: using utils.make_grid
writer.add_image('mnist_images', grid, 0)
print("\nLogged sample images to TensorBoard")

# Commented out IPython magic to ensure Python compatibility.
# Initialize the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Define the SimpleModel class first
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = nn.functional.relu(x)
        x = self.conv2(x)
        x = nn.functional.relu(x)
        x = nn.functional.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return x

# Now initialize the model, loss function, and optimizer
model = SimpleModel().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)

# Rest of your code...
# Log the model graph to TensorBoard
writer.add_graph(model, images.to(device))
print("Logged model graph to TensorBoard")

# Function to test on the validation set and log metrics
def test(epoch):
    model.eval()
    test_loss = 0
    correct = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader)
    test_acc = correct / len(test_dataset)

    # Log test metrics to TensorBoard
    writer.add_scalar('test loss', test_loss, epoch)
    writer.add_scalar('test accuracy', 100. * test_acc, epoch)

    return test_loss, test_acc

# Train for a few epochs
num_epochs = 100
print("\nðŸ“Š Training with TensorBoard Logging:")

for epoch in range(1, num_epochs + 1):
    train_acc = train_epoch(epoch)
    test_loss, test_acc = test(epoch)

    print(f"Epoch {epoch}: Train Acc: {train_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}")

# Now display TensorBoard directly in the notebook
print("\nLaunching TensorBoard in the notebook. This will visualize all the metrics we've logged.")
# %tensorboard --logdir runs

print("\n" + "="*80)
print("TUTORIAL COMPLETED!".center(80))
print("="*80)